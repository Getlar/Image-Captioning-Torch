{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHvW-wVxJtGA"
      },
      "source": [
        "# Download the Flickr8k dataset from Kaggle\n",
        "First create a token, download it and upload it here. Follow these steps: https://www.kaggle.com/discussions/general/74235"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWhd4vl0yUmg",
        "outputId": "d450927a-2b0e-4ad6-9613-40082c596dcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm\n",
            "  Downloading timm-0.9.12-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from timm) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.16.0+cu118)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.19.4)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (4.66.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (23.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.23.5)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7->timm) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7->timm) (1.3.0)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.9.12\n"
          ]
        }
      ],
      "source": [
        "!pip install -q kaggle\n",
        "!pip install timm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uy0rWmR74v1z"
      },
      "source": [
        "### Upload you Kaggle API Token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "Y8u6wiiHJc3H",
        "outputId": "3958001f-1818-44a0-dd3a-0a1c10cca1f9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2d6bdb43-6b31-4065-865c-2c20c36ea52a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2d6bdb43-6b31-4065-865c-2c20c36ea52a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"tamstakcs\",\"key\":\"142401ead91574da9cedb59435020939\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzNG6zLv4z5P"
      },
      "source": [
        "### Download Script for the Flickr8k dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-VyXo1wKFJe",
        "outputId": "18b15bfd-dc62-4b6f-e153-bb673a9413f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading flickr8k.zip to /content\n",
            "100% 1.04G/1.04G [00:05<00:00, 192MB/s]\n",
            "100% 1.04G/1.04G [00:05<00:00, 186MB/s]\n"
          ]
        }
      ],
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "!kaggle datasets download -d sayanf/flickr8k"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22hW4faj43lE"
      },
      "source": [
        "### Organizing annotations and images into separate folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dW5BnumKINN"
      },
      "outputs": [],
      "source": [
        "!unzip -q flickr8k.zip\n",
        "!mv Flickr8k_Dataset Flickr8k_images\n",
        "!mv Flickr8k_text Flickr8k_annotations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMmwjub8479s"
      },
      "source": [
        "### Downloading the GLOVE Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXkgpfG7BSLX",
        "outputId": "0e0a1c52-3407-4eee-b4de-28d65de62f96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-14 14:45:58--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2023-12-14 14:45:58--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.28MB/s    in 2m 50s  \n",
            "\n",
            "2023-12-14 14:48:49 (4.83 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://nlp.stanford.edu/data/glove.6B.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2SPO1GzB7YS",
        "outputId": "e35369ec-0c6d-44ad-865d-b9f417959dd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: Flickr8k_annotations/glove.6B.50d.txt  \n",
            "  inflating: Flickr8k_annotations/glove.6B.100d.txt  \n",
            "  inflating: Flickr8k_annotations/glove.6B.200d.txt  \n",
            "  inflating: Flickr8k_annotations/glove.6B.300d.txt  \n"
          ]
        }
      ],
      "source": [
        "!unzip -d Flickr8k_annotations/ glove.6B.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkVZUQk45IFg"
      },
      "source": [
        "### Download the images of our Test Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeUca5KnOtCU",
        "outputId": "73c07d5e-bb8c-4132-cd7c-a0dd6b566ef7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1QHpHBFH9glz1P_vddU2g8Pg4pxFWlCv-\n",
            "To: /content/test_dataset.zip\n",
            "100% 129M/129M [00:01<00:00, 70.6MB/s]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "if os.path.isdir('test_dataset')==False:\n",
        "    !gdown 1QHpHBFH9glz1P_vddU2g8Pg4pxFWlCv-\n",
        "    !unzip -qq test_dataset.zip -d test_dataset\n",
        "    !rm -rf test_dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDYlRrWEVP-0",
        "outputId": "cd1dec98-65c2-40b7-cf41-d3ddfe37a1f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of images in the test dataset: 176 - 1\n"
          ]
        }
      ],
      "source": [
        "!echo \"Number of images in the test dataset: $(ls -l test_dataset/test_dataset/ | wc -l) - 1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALmMEVFr6mc1"
      },
      "source": [
        "### Downloading the captions for the images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-Zz_Sj_SX5E",
        "outputId": "fbf6e5b3-5ebf-419f-9cbc-6c81d553eb56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1KQhnrfWtPfXbApXHXGEiz6rGbGa4NdDJ\n",
            "To: /content/test_captions_tokenized.txt\n",
            "\r  0% 0.00/60.0k [00:00<?, ?B/s]\r100% 60.0k/60.0k [00:00<00:00, 102MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown 1KQhnrfWtPfXbApXHXGEiz6rGbGa4NdDJ\n",
        "!mv test_captions_tokenized.txt Flickr8k_annotations/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqXv_msr85yz",
        "outputId": "dc9435c0-2ccc-4faf-c22b-6de1e8cdf927"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of captions in the test dataset: 875\n",
            "875\n"
          ]
        }
      ],
      "source": [
        "!echo \"Number of captions in the test dataset: $(wc -l < Flickr8k_annotations/test_captions_tokenized.txt)\"\n",
        "!echo $((175 * 5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n94CBrDlU5q7",
        "outputId": "153c01b6-c60b-42e2-fad6-f4447f251712"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of images in the dataset: 8092\n"
          ]
        }
      ],
      "source": [
        "!echo \"Number of images in the dataset: $(ls -l Flickr8k_images/ | wc -l)\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoWFwhf867V8"
      },
      "source": [
        "### Add the test images to the image folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RhYLwSDRxet"
      },
      "outputs": [],
      "source": [
        "!mv test_dataset/test_dataset/*.jpg Flickr8k_images/\n",
        "!mv test_dataset/test_dataset/*.png Flickr8k_images/\n",
        "!mv test_dataset/test_dataset/*.jpeg Flickr8k_images/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xanlPazEVGSd",
        "outputId": "f89f02a8-83a2-4249-f1d3-a441b57be293"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of images in the train dataset after adding test images: 8267\n"
          ]
        }
      ],
      "source": [
        "!echo \"Number of images in the train dataset after adding test images: $(ls -l Flickr8k_images/ | wc -l)\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkmxlUvv7R4M"
      },
      "source": [
        "### Remove all unnecessary directories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7IohBem6nhy"
      },
      "outputs": [],
      "source": [
        "!rm -rf sample_data/\n",
        "!rm -f flickr8k.zip\n",
        "!rm -f glove.6B.zip\n",
        "!rm -rf test_dataset/\n",
        "!rm -f kaggle.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8yRZ1yI_Qel"
      },
      "source": [
        "# Necessary imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhIrMt6XKwBj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import timm\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torch.nn.functional as F\n",
        "import string\n",
        "import nltk\n",
        "import shutil\n",
        "import time\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import TransformerDecoderLayer, TransformerDecoder\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from torch.utils.data import DataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0h-wtiI-KL6"
      },
      "outputs": [],
      "source": [
        "## CONSTANTS\n",
        "\n",
        "EMBEDDING_SIZE = 300 # Used by GLOVE\n",
        "MAX_LEN = 60\n",
        "\n",
        "START_IDX = 1\n",
        "END_IDX = 2\n",
        "PAD_IDX = 0\n",
        "UNK_IDX = 3\n",
        "\n",
        "START_TOKEN = \"<start>\"\n",
        "END_TOKEN = \"<end>\"\n",
        "PAD_TOKEN = \"<pad>\"\n",
        "UNK_TOKEN = \"<unk>\"\n",
        "\n",
        "## TRAINING HYPERPARAMS\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "SHUFFLE = True\n",
        "NUM_WORKERS = 1\n",
        "DROP_LAST = True\n",
        "NUM_OF_EPOCHS = 20\n",
        "L2_PENALTY = 0.5\n",
        "LEARNING_RATE = 0.000008\n",
        "GRADIENT_CLIPPING = 2.0\n",
        "EVAL_PERIOD = 1\n",
        "\n",
        "## MODEL\n",
        "\n",
        "IMG_FEATURE_CHANNELS = 2048\n",
        "IMG_SIZE = 256\n",
        "DECODER_LAYERS = 8\n",
        "D_MODEL = 512\n",
        "FF_DIM = 1024\n",
        "ATTENTION_HEADS = 16\n",
        "DROPOUT = 0.5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQUBZEyv8WIR"
      },
      "outputs": [],
      "source": [
        "def load_captions(data):\n",
        "    image2caption = dict()\n",
        "    for sample in data.split(\"\\n\"):\n",
        "        tokens = sample.split()\n",
        "        if len(sample) < 2:\n",
        "            continue\n",
        "        image_name, image_caption = tokens[0], tokens[1:]\n",
        "\n",
        "        image_id = image_name.split(\".\")[0]\n",
        "        image_caption = \" \".join(image_caption)\n",
        "\n",
        "        if image_id not in image2caption:\n",
        "            image2caption[image_id] = list()\n",
        "        image2caption[image_id].append(image_caption)\n",
        "\n",
        "    return image2caption"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8P0M1jPW9X4T"
      },
      "outputs": [],
      "source": [
        "def preprocess_caption(caption):\n",
        "    punct_table = str.maketrans(\"\", \"\", string.punctuation)\n",
        "    # Extract separate tokens\n",
        "    caption = caption.split()\n",
        "    # Make tokens lowercase\n",
        "    caption = [word.lower() for word in caption]\n",
        "    # Remove punctuation\n",
        "    caption = [word.translate(punct_table) for word in caption]\n",
        "    # Remove trailing \"'s\" or \"a\"\n",
        "    caption = [word for word in caption if len(word) > 1]\n",
        "    # Remove tokens which contain number\n",
        "    caption = [word for word in caption if word.isalpha()]\n",
        "    return \" \".join(caption)\n",
        "\n",
        "\n",
        "def clean_captions(id2annotation):\n",
        "    image2caption_clean = id2annotation.copy()\n",
        "    for image_id, captions in id2annotation.items():\n",
        "        for i in range(len(captions)):\n",
        "            caption = captions[i]\n",
        "            clean_caption = preprocess_caption(caption)\n",
        "            image2caption_clean[image_id][i] =  clean_caption\n",
        "\n",
        "    return image2caption_clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgMQ8r6Q-w2B"
      },
      "outputs": [],
      "source": [
        "def create_vocab(image2caption):\n",
        "    word2idx = {\"<pad>\": 0, \"<start>\": 1, \"<end>\": 2, \"<unk>\": 3}\n",
        "    words = set()\n",
        "    for captions in image2caption.values():\n",
        "        current_words = [word for caption in captions for word in caption.split()]\n",
        "        words.update(current_words)\n",
        "\n",
        "    starting_len = len(word2idx)\n",
        "    words = list(words)\n",
        "    word2idx.update({word: (idx + starting_len) for idx, word in enumerate(words)})\n",
        "\n",
        "    return word2idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UrTAWIA_sya"
      },
      "outputs": [],
      "source": [
        "def extract_embeddings(vocab):\n",
        "    np.random.seed(20231104)\n",
        "    glove_dir = \"Flickr8k_annotations\"\n",
        "    embeddings_config = {\n",
        "        \"path\": \"Flickr8k_annotations/embeddings.txt\",\n",
        "        \"size\": EMBEDDING_SIZE\n",
        "    }\n",
        "    save_path_emb = embeddings_config[\"path\"]\n",
        "    embedding_dim = embeddings_config[\"size\"]\n",
        "\n",
        "    punct_table = str.maketrans(\"\", \"\", string.punctuation)\n",
        "\n",
        "    vectors = []\n",
        "    new_vocab = {\"<pad>\": 0, \"<start>\": 1, \"<end>\": 2, \"<unk>\": 3}\n",
        "    i = len(new_vocab)\n",
        "\n",
        "    embedding_file_name = \"glove.6B.{}d.txt\".format(embedding_dim)\n",
        "    embeddings_path = os.path.join(glove_dir, embedding_file_name)\n",
        "    with open(embeddings_path, \"rb\") as f:\n",
        "        for line in f:\n",
        "            line = line.decode().split()\n",
        "            word = line[0]\n",
        "            word = word.strip().lower()\n",
        "            word = word.translate(punct_table)\n",
        "            if word in vocab and word not in new_vocab:\n",
        "                embedding_vec = np.array(line[1:], dtype=\"float\")\n",
        "                vectors += [embedding_vec]\n",
        "                new_vocab[word] = i\n",
        "                i += 1\n",
        "    with open(\"Flickr8k_annotations/word2idx.json\", \"w\", encoding=\"utf8\") as f:\n",
        "        json.dump(new_vocab, f)\n",
        "\n",
        "    vectors = np.array(vectors)\n",
        "    # Embedding vector for tokens used for padding the input sequence\n",
        "    pad_embedding = np.zeros((embedding_dim,))\n",
        "    # Embedding vector for start of the sequence\n",
        "    sos_embedding = np.random.normal(size=(embedding_dim,))\n",
        "    # Embedding vector for end of the sequence\n",
        "    eos_embedding = np.random.normal(size=(embedding_dim,))\n",
        "    # Embedding vector for unknown token\n",
        "    unk_embedding =  np.random.normal(size=(embedding_dim,))\n",
        "\n",
        "    assert not np.allclose(sos_embedding, eos_embedding), \"SOS and EOS embeddings are too close!\"\n",
        "    for emb_vec in vectors:\n",
        "        assert not np.allclose(sos_embedding, emb_vec), \"SOS embedding is too close to other embedding!\"\n",
        "        assert not np.allclose(eos_embedding, emb_vec), \"EOS embedding is too close to other embedding!\"\n",
        "\n",
        "\n",
        "    print(\"Embedding vectors shape without SOS EOS UNK and PAD: \", vectors.shape)\n",
        "    vectors = np.vstack([pad_embedding, sos_embedding, eos_embedding, unk_embedding, vectors])\n",
        "\n",
        "    print(\"Embedding vectors shape having added SOS EOS UNK and PAD: \", vectors.shape)\n",
        "    np.savetxt(save_path_emb, vectors)\n",
        "\n",
        "    print(\"\\nExtracted GloVe embeddings for all tokens in the training set.\")\n",
        "    print(\"Embedding vectors size:\", embedding_dim)\n",
        "    print(\"Vocab size:\", len(new_vocab))\n",
        "    return new_vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEJZB_wsF4_8"
      },
      "outputs": [],
      "source": [
        "def save_captions(image2caption, subset_imgs, save_path):\n",
        "    captions = []\n",
        "    for image_name in subset_imgs:\n",
        "        image_id = os.path.splitext(image_name)[0]\n",
        "        if image_id in image2caption:\n",
        "            for caption in image2caption[image_id]:\n",
        "                captions.append(\"{} {}\\n\".format(image_name, caption))\n",
        "\n",
        "    with open(save_path, \"w\") as f:\n",
        "        f.writelines(captions)\n",
        "\n",
        "def split_dataset(image2caption, split_images_paths, save_paths):\n",
        "    for load_path, save_path in zip(split_images_paths, save_paths):\n",
        "        with open(load_path, \"r\") as f:\n",
        "            subset_imgs = [fname.replace(\"\\n\", \"\") for fname in f.readlines()]\n",
        "        save_captions(image2caption, subset_imgs, save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuoYZ36SWNzz",
        "outputId": "83cb9d64-92ad-40af-ce4f-2aaab9e54811"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of images in the train dataset: 6000\n",
            "Number of images in the test dataset: 1000\n"
          ]
        }
      ],
      "source": [
        "!echo \"Number of images in the train dataset: $(cat Flickr8k_annotations/Flickr_8k.trainImages.txt | wc -l)\"\n",
        "!echo \"Number of images in the test dataset: $(cat Flickr8k_annotations/Flickr_8k.testImages.txt | wc -l)\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scr6PmM4_u8h"
      },
      "source": [
        "### Add the test images to the training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJ-HlhaNVlbs"
      },
      "outputs": [],
      "source": [
        "source_path = \"Flickr8k_annotations/Flickr_8k.testImages.txt\"\n",
        "destination_path = \"Flickr8k_annotations/Flickr_8k.trainImages.txt\"\n",
        "\n",
        "with open(destination_path, \"a\") as f_dest, open(source_path, \"r\") as f_source:\n",
        "    shutil.copyfileobj(f_source, f_dest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pn-jQOAFWYd1",
        "outputId": "3df335c3-92f4-4589-b13f-357baf5f5a53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of images in the test dataset after adding training images: 7000\n"
          ]
        }
      ],
      "source": [
        "!echo \"Number of images in the test dataset after adding training images: $(cat Flickr8k_annotations/Flickr_8k.trainImages.txt | wc -l)\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"Number of lines in the tokenized dataset: $(cat Flickr8k_annotations/Flickr8k.token.txt | wc -l)\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwNmqOl16nkB",
        "outputId": "184b0552-e305-4365-a750-4b70e4f2b09a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of lines in the tokenized dataset: 40460\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"Number of lines in our tokenized dataset: $(cat Flickr8k_annotations/test_captions_tokenized.txt | wc -l)\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpiXYRGP6sFm",
        "outputId": "f4c93caf-8cc8-4677-de66-6800fb48b833"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of lines in our tokenized dataset: 875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ng3bNEUJ_4Ve"
      },
      "source": [
        "#### Removes test image IDs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZndpSjkWYPXi"
      },
      "outputs": [],
      "source": [
        "!rm Flickr8k_annotations/Flickr_8k.testImages.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpclM_5l_99y"
      },
      "source": [
        "#### Add all unique image IDs from our test set and add it to the test IDs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qE6FyReTWfNl"
      },
      "outputs": [],
      "source": [
        "!sed -n 's/\\([^#]\\+\\)\\(jpg\\|jpeg\\|png\\).*/\\1\\2/p' Flickr8k_annotations/test_captions_tokenized.txt | uniq > Flickr8k_annotations/Flickr_8k.testImages.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEAGnVuITV66",
        "outputId": "a5211234-4ee5-4428-df29-7cb1f6ab8070"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of images in our test dataset: 175\n"
          ]
        }
      ],
      "source": [
        "!echo \"Number of images in our test dataset: $(cat Flickr8k_annotations/Flickr_8k.testImages.txt | wc -l)\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIhCE7c2IPLM"
      },
      "source": [
        "### Open the tokenized captions of Flickr Images\n",
        "\n",
        "#### Create a image2caption (dict): Mapping from image id to all captions of that image that occured in the datase\n",
        "\n",
        "#### Clean the captions, remove stopwords etc.\n",
        "\n",
        "#### Create a vocabulary of used words\n",
        "\n",
        "#### Extract the embeddings for words from GLOVE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yudqSvO6IOHM",
        "outputId": "f634785e-16d4-4ed7-c1a0-c27c59fff8d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding vectors shape without SOS EOS UNK and PAD:  (7886, 300)\n",
            "Embedding vectors shape having added SOS EOS UNK and PAD:  (7890, 300)\n",
            "\n",
            "Extracted GloVe embeddings for all tokens in the training set.\n",
            "Embedding vectors size: 300\n",
            "Vocab size: 7890\n"
          ]
        }
      ],
      "source": [
        "dataset_path = \"Flickr8k_annotations/Flickr8k.token.txt\"\n",
        "with open(dataset_path, \"r\") as f:\n",
        "  data = f.read()\n",
        "\n",
        "image2caption = load_captions(data)\n",
        "image2caption = clean_captions(image2caption)\n",
        "vocab = create_vocab(image2caption)\n",
        "\n",
        "new_vocab = extract_embeddings(vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvNU5fZkAGwm"
      },
      "source": [
        "#### Create the image-caption data for the network: line of (image_id caption[i]) * 5 for every image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAw5TBW5VjMu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34c23afa-f8db-4183-eb90-124dea477a47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Flickr8k_annotations/Flickr_8k.trainImages.txt', 'Flickr8k_annotations/Flickr_8k.devImages.txt']\n"
          ]
        }
      ],
      "source": [
        "split_images = {\n",
        "  \"train\": \"Flickr8k_annotations/Flickr_8k.trainImages.txt\",\n",
        "  \"validation\": \"Flickr8k_annotations/Flickr_8k.devImages.txt\",\n",
        "  \"test\" : \"Flickr8k_annotations/Flickr_8k.testImages.txt\",\n",
        "}\n",
        "\n",
        "split_save = {\n",
        "  \"train\": \"Flickr8k_annotations/train.txt\",\n",
        "  \"validation\": \"Flickr8k_annotations/validation.txt\",\n",
        "  \"test\": \"Flickr8k_annotations/test.txt\",\n",
        "}\n",
        "\n",
        "split_images_paths = list(split_images.values())[:-1]\n",
        "split_save_paths = list(split_save.values())[:-1]\n",
        "\n",
        "print(split_images_paths)\n",
        "\n",
        "split_dataset(image2caption, split_images_paths, split_save_paths)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = \"Flickr8k_annotations/test_captions_tokenized.txt\"\n",
        "with open(dataset_path, \"r\") as f:\n",
        "    data_test = f.read()\n",
        "\n",
        "image2caption_test = load_captions(data_test)\n",
        "image2caption_test = clean_captions(image2caption_test)\n",
        "\n",
        "split_images_paths_test = list(split_images.values())[-1]\n",
        "split_save_paths_test = list(split_save.values())[-1]\n",
        "\n",
        "split_dataset(image2caption_test, [split_images_paths_test], [split_save_paths_test])"
      ],
      "metadata": {
        "id": "YlK9BWZg08q6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i65nxMUwAmM_"
      },
      "source": [
        "#### Set up GPU device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKlXmcB7ZL7j",
        "outputId": "795d6587-f5f2-41ef-d662-c38e384a76c0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "use_gpu = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_gpu else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enHsNrN1A_IX"
      },
      "source": [
        "### Create The Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93760zz50Otb"
      },
      "outputs": [],
      "source": [
        "class Flickr8KDataset(Dataset):\n",
        "\n",
        "    def __init__(self, path, type_data = \"train\"):\n",
        "        with open(path, \"r\") as f:\n",
        "            self._data = [line.replace(\"\\n\", \"\") for line in f.readlines()]\n",
        "\n",
        "        self._inference_captions = self._group_captions(self._data)\n",
        "        self._type = type_data\n",
        "\n",
        "        with open(\"Flickr8k_annotations/word2idx.json\", \"r\", encoding=\"utf8\") as f:\n",
        "            self._word2idx = json.load(f)\n",
        "        self._idx2word = {str(idx): word for word, idx in self._word2idx.items()}\n",
        "\n",
        "        self._start_idx = START_IDX\n",
        "        self._end_idx = END_IDX\n",
        "        self._pad_idx = PAD_IDX\n",
        "        self._UNK_idx = UNK_IDX\n",
        "        self._START_token = START_TOKEN\n",
        "        self._END_token = END_TOKEN\n",
        "        self._PAD_token = PAD_TOKEN\n",
        "        self._UNK_token = UNK_TOKEN\n",
        "\n",
        "        self._max_len = MAX_LEN\n",
        "\n",
        "        self._img_feature_channels = IMG_FEATURE_CHANNELS\n",
        "        self._img_size = IMG_SIZE\n",
        "\n",
        "        self._image_transform = self._construct_image_transform(self._img_size)\n",
        "        self.image_dir = \"Flickr8k_images\"\n",
        "\n",
        "        self._data = self._create_input_label_mappings(self._data)\n",
        "\n",
        "        self._dataset_size = len(self._data)\n",
        "\n",
        "    def _construct_image_transform(self, image_size):\n",
        "        normalize = transforms.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225]\n",
        "        )\n",
        "        preprocessing = transforms.Compose([\n",
        "            transforms.Resize(image_size),\n",
        "            transforms.CenterCrop(image_size),\n",
        "            transforms.ToTensor(),\n",
        "            normalize,\n",
        "        ])\n",
        "        return preprocessing\n",
        "\n",
        "    def _group_captions(self, data):\n",
        "        grouped_captions = {}\n",
        "\n",
        "        for line in data:\n",
        "            caption_data = line.split()\n",
        "            img_name, img_caption = caption_data[0].split(\"#\")[0], caption_data[1:]\n",
        "            if img_name not in grouped_captions:\n",
        "                grouped_captions[img_name] = []\n",
        "\n",
        "            grouped_captions[img_name].append(img_caption)\n",
        "\n",
        "        return grouped_captions\n",
        "\n",
        "    def _create_input_label_mappings(self, data):\n",
        "        processed_data = []\n",
        "        for line in data:\n",
        "            tokens = line.split()\n",
        "            img_name, caption_words = tokens[0].split(\"#\")[0], tokens[1:]\n",
        "            pair = (img_name, caption_words)\n",
        "            processed_data.append(pair)\n",
        "\n",
        "        return processed_data\n",
        "\n",
        "    def _load_and_prepare_image(self, image_name):\n",
        "        image_path = os.path.join(self.image_dir, image_name)\n",
        "        raw_image = Image.open(image_path).convert(\"RGB\")\n",
        "        image_tensors = self._image_transform(raw_image)\n",
        "        return image_tensors\n",
        "\n",
        "    def inference_batch(self, batch_size):\n",
        "        caption_data_items = list(self._inference_captions.items())\n",
        "\n",
        "        num_batches = len(caption_data_items) // batch_size\n",
        "        for idx in range(num_batches):\n",
        "            caption_samples = caption_data_items[idx * batch_size: (idx + 1) * batch_size]\n",
        "            batch_imgs = []\n",
        "            batch_captions = []\n",
        "\n",
        "            idx += batch_size\n",
        "\n",
        "            for image_name, captions in caption_samples:\n",
        "                batch_captions.append(captions)\n",
        "                image_tensor = self._load_and_prepare_image(image_name)\n",
        "                batch_imgs.append(image_tensor)\n",
        "\n",
        "\n",
        "            batch_imgs = torch.stack(batch_imgs, dim=0)\n",
        "            if batch_size == 1:\n",
        "                batch_imgs = batch_imgs.squeeze(0)\n",
        "\n",
        "            yield batch_imgs, batch_captions\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._dataset_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image_id, tokens = self._data[index]\n",
        "\n",
        "\n",
        "        image_tensor = self._load_and_prepare_image(image_id)\n",
        "\n",
        "\n",
        "        tokens = tokens[:self._max_len]\n",
        "\n",
        "        tokens = [token.strip().lower() for token in tokens]\n",
        "        tokens = [self._START_token] + tokens + [self._END_token]\n",
        "\n",
        "        input_tokens = tokens[:-1].copy()\n",
        "        tgt_tokens = tokens[1:].copy()\n",
        "\n",
        "        sample_size = len(input_tokens)\n",
        "        padding_size = self._max_len - sample_size\n",
        "\n",
        "        if padding_size > 0:\n",
        "            padding_vec = [self._PAD_token for _ in range(padding_size)]\n",
        "            input_tokens += padding_vec.copy()\n",
        "            tgt_tokens += padding_vec.copy()\n",
        "\n",
        "        input_tokens = [self._word2idx.get(token, self._UNK_idx) for token in input_tokens]\n",
        "        tgt_tokens = [self._word2idx.get(token, self._UNK_idx) for token in tgt_tokens]\n",
        "\n",
        "        input_tokens = torch.Tensor(input_tokens).long()\n",
        "        tgt_tokens = torch.Tensor(tgt_tokens).long()\n",
        "\n",
        "        tgt_padding_mask = torch.ones([self._max_len, ])\n",
        "        tgt_padding_mask[:sample_size] = 0.0\n",
        "        tgt_padding_mask = tgt_padding_mask.bool()\n",
        "\n",
        "        return image_tensor, input_tokens, tgt_tokens, tgt_padding_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sniKcsICbEey",
        "outputId": "6147bf91-286d-48d4-a6a3-251821363bf4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of image:caption pairs in the train set:  35000\n",
            "Number of image:caption pairs in the validation set:  5000\n"
          ]
        }
      ],
      "source": [
        "train_set = Flickr8KDataset(split_save[\"train\"], \"train\")\n",
        "valid_set = Flickr8KDataset(split_save[\"validation\"], \"valid\")\n",
        "\n",
        "print(\"Number of image:caption pairs in the train set: \", len(train_set))\n",
        "print(\"Number of image:caption pairs in the validation set: \", len(valid_set))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wqBNGChbkFX"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=SHUFFLE, num_workers=NUM_WORKERS, drop_last=DROP_LAST)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxW99_ymdfp5"
      },
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Linear(input_dim, input_dim),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(input_dim, input_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        skip_connection = x\n",
        "        x = self.block(x)\n",
        "        x = skip_connection + x\n",
        "        return x\n",
        "\n",
        "\n",
        "class Normalize(nn.Module):\n",
        "    def __init__(self, eps=1e-5):\n",
        "        super(Normalize, self).__init__()\n",
        "        self.register_buffer(\"eps\", torch.Tensor([eps]))\n",
        "\n",
        "    def forward(self, x, dim=-1):\n",
        "        norm = x.norm(2, dim=dim).unsqueeze(-1)\n",
        "        x = self.eps * (x / norm)\n",
        "        return x\n",
        "\n",
        "\n",
        "class PositionalEncodings(nn.Module):\n",
        "\n",
        "    def __init__(self, seq_len, d_model, p_dropout):\n",
        "        super(PositionalEncodings, self).__init__()\n",
        "        token_positions = torch.arange(start=0, end=seq_len).view(-1, 1)\n",
        "        dim_positions = torch.arange(start=0, end=d_model).view(1, -1)\n",
        "        angles = token_positions / (10000 ** ((2 * dim_positions) / d_model))\n",
        "\n",
        "        encodings = torch.zeros(1, seq_len, d_model)\n",
        "        encodings[0, :, ::2] = torch.cos(angles[:, ::2])\n",
        "        encodings[0, :, 1::2] = torch.sin(angles[:, 1::2])\n",
        "        encodings.requires_grad = False\n",
        "        self.register_buffer(\"positional_encodings\", encodings)\n",
        "\n",
        "        self.dropout = nn.Dropout(p_dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.positional_encodings\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, train=False):\n",
        "        super(Encoder, self).__init__()\n",
        "        resnet = timm.create_model('seresnet152d', pretrained=True)\n",
        "        modules = list(resnet.children())[:-2]\n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "        if not train:\n",
        "          for p in self.resnet.parameters():\n",
        "              p.requires_grad = False\n",
        "\n",
        "    def forward(self, images):\n",
        "        out = self.resnet(images)  # (batch_size, 2048, image_size/32, image_size/32)\n",
        "        out = out.view(out.size(0), out.size(1), -1)\n",
        "        out = out.permute(0, 2, 1)\n",
        "        return out\n",
        "\n",
        "\n",
        "class CaptionDecoder(nn.Module):\n",
        "\n",
        "    def __init__(self, decoder_layers, d_model, ff_dim, attention_heads, dropout, embedding_dim, img_feature_channels, word_embeddings, vocab_size, device):\n",
        "        super(CaptionDecoder, self).__init__()\n",
        "\n",
        "        self.embedding_layer = nn.Embedding.from_pretrained(\n",
        "            word_embeddings,\n",
        "            freeze=True,\n",
        "            padding_idx=0\n",
        "        )\n",
        "\n",
        "        self.entry_mapping_words = nn.Linear(embedding_dim, d_model)\n",
        "        self.entry_mapping_img = nn.Linear(img_feature_channels, d_model)\n",
        "\n",
        "        self.res_block = ResidualBlock(d_model)\n",
        "\n",
        "        self.positional_encodings = PositionalEncodings(60, d_model, dropout)\n",
        "        transformer_decoder_layer = TransformerDecoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=attention_heads,\n",
        "            dim_feedforward=ff_dim,\n",
        "            dropout=dropout\n",
        "        )\n",
        "        self.decoder = TransformerDecoder(transformer_decoder_layer, decoder_layers)\n",
        "        self.classifier = nn.Linear(d_model, vocab_size)\n",
        "        self.set_up_causal_mask(MAX_LEN, device)\n",
        "\n",
        "    def set_up_causal_mask(self, seq_len, device):\n",
        "        self.casual_mask = (torch.triu(torch.ones(seq_len, seq_len)) == 1).transpose(0, 1)\n",
        "        self.casual_mask = self.casual_mask.float().masked_fill(self.casual_mask == 0, float('-inf')).masked_fill(self.casual_mask == 1, float(0.0)).to(device)\n",
        "        self.casual_mask.requires_grad = False\n",
        "\n",
        "    def forward(self, x, image_features, tgt_padding_mask=None):\n",
        "        image_features = self.entry_mapping_img(image_features)\n",
        "        image_features = image_features.permute(1, 0, 2)\n",
        "        image_features = F.leaky_relu(image_features)\n",
        "\n",
        "        x = self.embedding_layer(x)\n",
        "        x = self.entry_mapping_words(x)\n",
        "        x = F.leaky_relu(x)\n",
        "\n",
        "        x = self.res_block(x)\n",
        "        x = F.leaky_relu(x)\n",
        "\n",
        "        x = self.positional_encodings(x)\n",
        "\n",
        "        x = x.permute(1, 0, 2)\n",
        "\n",
        "        tgt_padding_mask = tgt_padding_mask.type(torch.bool)\n",
        "        self.casual_mask = self.casual_mask.type(torch.bool)\n",
        "\n",
        "        x = self.decoder(\n",
        "            tgt=x,\n",
        "            memory=image_features,\n",
        "            tgt_key_padding_mask=tgt_padding_mask,\n",
        "            tgt_mask=self.casual_mask\n",
        "        )\n",
        "        x = x.permute(1, 0, 2)\n",
        "\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "class CNNtoTransformer(nn.Module):\n",
        "\n",
        "    def __init__(self, decoder_layers, d_model, ff_dim, attention_heads, dropout, embedding_dim, img_feature_channels, vocab_size, device):\n",
        "        super(CNNtoTransformer, self).__init__()\n",
        "\n",
        "        self.encoderCNN = Encoder()\n",
        "        self.encoderCNN.eval()\n",
        "\n",
        "        word_embeddings = torch.Tensor(np.loadtxt(\"Flickr8k_annotations/embeddings.txt\"))\n",
        "        self.decoderRNN = CaptionDecoder(decoder_layers, d_model, ff_dim, attention_heads, dropout, embedding_dim, img_feature_channels, word_embeddings, vocab_size, device)\n",
        "\n",
        "    def forward(self, images, x_words, tgt_padding_mask=None):\n",
        "        image_features = self.encoderCNN(images)\n",
        "        outputs = self.decoderRNN(x_words, image_features, tgt_padding_mask)\n",
        "        return outputs\n",
        "\n",
        "    def forward_encoder(self, images):\n",
        "        return self.encoderCNN(images)\n",
        "\n",
        "    def forward_decoder(self, x_words, image_features, tgt_padding_mask):\n",
        "        return self.decoderRNN(x_words, image_features, tgt_padding_mask)\n",
        "\n",
        "    def set_encoder_train_mode(self, mode=True):\n",
        "        if mode:\n",
        "            self.encoderCNN.train()\n",
        "        else:\n",
        "            self.encoderCNN.eval()\n",
        "\n",
        "    def set_decoder_train_mode(self, mode=True):\n",
        "        if mode:\n",
        "            self.decoderRNN.train()\n",
        "        else:\n",
        "            self.decoderRNN.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9rMFW1Dbp_h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "ad6de0cf34b94316ac2e2888593fbc7e",
            "781d9716a4d646f0ab70d24e2823dc1e",
            "5903abd570ad4eacb9ddff56b4ae7597",
            "819866ddf5084386a2e0b7123d640401",
            "797b8c938cf4439ebdcd26b4a6ebfccb",
            "ee406212b1c04378b78919efb593d545",
            "07345f587d9948aea1502889f0c7c0d6",
            "a8e3772673a94d4fab122e738e3c7ecb",
            "408540d767b444898c104a2fc73ecc94",
            "602bc65c29534df2b1acde44608c86cb",
            "c7a9d0a99bcc42378881616cb629818d"
          ]
        },
        "outputId": "17b326e3-1ab7-4cdc-c086-693e0f36290d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ad6de0cf34b94316ac2e2888593fbc7e"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "encoder_decoder = CNNtoTransformer(\n",
        "    decoder_layers = DECODER_LAYERS,\n",
        "    d_model = D_MODEL,\n",
        "    ff_dim = FF_DIM,\n",
        "    attention_heads = ATTENTION_HEADS,\n",
        "    dropout = DROPOUT,\n",
        "    embedding_dim = EMBEDDING_SIZE,\n",
        "    img_feature_channels = IMG_FEATURE_CHANNELS,\n",
        "    vocab_size = len(new_vocab),\n",
        "    device = device\n",
        "    )\n",
        "\n",
        "encoder_decoder = encoder_decoder.to(device)\n",
        "encoder_decoder.set_encoder_train_mode(False)\n",
        "encoder_decoder.set_decoder_train_mode(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQHU4JgGfEXG"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(\n",
        "    encoder_decoder.parameters(),\n",
        "    lr=LEARNING_RATE,\n",
        "    weight_decay=L2_PENALTY\n",
        ")\n",
        "loss_fcn = nn.CrossEntropyLoss(label_smoothing=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qgcc3uPRgBaT"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(model, optimizer, start_time, epoch):\n",
        "\n",
        "    target_dir = os.path.join(\"checkpoints\", str(start_time))\n",
        "    os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "    save_path_model = os.path.join(target_dir, f\"model_{epoch}.pth\")\n",
        "    save_path_optimizer = os.path.join(target_dir, f\"optimizer_{epoch}.pth\")\n",
        "    torch.save(model.state_dict(), save_path_model)\n",
        "    torch.save(optimizer.state_dict(), save_path_optimizer)\n",
        "    print(\"Model saved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HyOm_c-x_D2Y"
      },
      "outputs": [],
      "source": [
        "def greedy_decoding(model, img_features_batched, sos_id, eos_id, pad_id, idx2word, max_len, device):\n",
        "    batch_size = img_features_batched.size(0)\n",
        "\n",
        "    # Define the initial state of decoder input\n",
        "    x_words = torch.Tensor([sos_id] + [pad_id] * (max_len - 1)).to(device).long()\n",
        "    x_words = x_words.repeat(batch_size, 1)\n",
        "    padd_mask = torch.Tensor([True] * max_len).to(device).bool()\n",
        "    padd_mask = padd_mask.repeat(batch_size, 1)\n",
        "\n",
        "    # Is each image from the batch decoded\n",
        "    is_decoded = [False] * batch_size\n",
        "    generated_captions = []\n",
        "    for _ in range(batch_size):\n",
        "        generated_captions.append([])\n",
        "\n",
        "    for i in range(max_len - 1):\n",
        "        # Update the padding masks\n",
        "        padd_mask[:, i] = False\n",
        "\n",
        "        # Get the model prediction for the next word\n",
        "        y_pred_prob = model.forward_decoder(x_words, img_features_batched, padd_mask)\n",
        "        # Extract the prediction from the specific (next word) position of the target sequence\n",
        "        y_pred_prob = y_pred_prob[torch.arange(batch_size), [i] * batch_size].clone()\n",
        "        # Extract the most probable word\n",
        "        y_pred = y_pred_prob.argmax(-1)\n",
        "\n",
        "        for batch_idx in range(batch_size):\n",
        "            if is_decoded[batch_idx]:\n",
        "                continue\n",
        "            # Add the generated word to the caption\n",
        "            generated_captions[batch_idx].append(idx2word[str(y_pred[batch_idx].item())])\n",
        "            if y_pred[batch_idx] == eos_id:\n",
        "                # Caption has been fully generated for this image\n",
        "                is_decoded[batch_idx] = True\n",
        "\n",
        "        if np.all(is_decoded):\n",
        "            break\n",
        "\n",
        "        if i < (max_len - 1):   # We haven't reached maximum number of decoding steps\n",
        "            # Update the input tokens for the next iteration\n",
        "            x_words[torch.arange(batch_size), [i+1] * batch_size] = y_pred.view(-1)\n",
        "\n",
        "    # Complete the caption for images which haven't been fully decoded\n",
        "    for batch_idx in range(batch_size):\n",
        "        if not is_decoded[batch_idx]:\n",
        "            generated_captions[batch_idx].append(idx2word[str(eos_id)])\n",
        "\n",
        "    # Clean the EOS symbol\n",
        "    for caption in generated_captions:\n",
        "        caption.remove(\"<end>\")\n",
        "\n",
        "    return generated_captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBvOQLY4gXP7"
      },
      "outputs": [],
      "source": [
        "def evaluate(subset, encoder_decoder, device):\n",
        "    \"\"\"Evaluates (BLEU score) caption generation model on a given subset.\n",
        "\n",
        "    Arguments:\n",
        "        subset (Flickr8KDataset): Train/Val/Test subset\n",
        "        encoder (nn.Module): CNN which generates image features\n",
        "        decoder (nn.Module): Transformer Decoder which generates captions for images\n",
        "        config (object): Contains configuration for the evaluation pipeline\n",
        "        device (torch.device): Device on which to port used tensors\n",
        "    Returns:\n",
        "        bleu (float): BLEU-{1:4} scores performance metric on the entire subset - corpus bleu\n",
        "    \"\"\"\n",
        "    max_len = 60\n",
        "    batch_size = 32\n",
        "    bleu_w = {\n",
        "      \"bleu-1\": [1.0],\n",
        "      \"bleu-2\": [0.5, 0.5],\n",
        "      \"bleu-3\": [0.333, 0.333, 0.333],\n",
        "      \"bleu-4\": [0.25, 0.25, 0.25, 0.25]\n",
        "    }\n",
        "\n",
        "    idx2word = subset._idx2word\n",
        "\n",
        "    sos_id = subset._start_idx\n",
        "    eos_id = subset._end_idx\n",
        "    pad_id = subset._pad_idx\n",
        "\n",
        "    references_total = []\n",
        "    predictions_total = []\n",
        "\n",
        "    print(\"Evaluating model.\")\n",
        "    for x_img_batched, y_caption_batched in subset.inference_batch(batch_size):\n",
        "\n",
        "        x_img_batched = x_img_batched.to(device)\n",
        "        img_features = encoder_decoder.forward_encoder(x_img_batched)\n",
        "        img_features = img_features.detach()\n",
        "\n",
        "        predictions = greedy_decoding(encoder_decoder, img_features, sos_id, eos_id, pad_id, idx2word, max_len, device)\n",
        "\n",
        "        references_total += y_caption_batched\n",
        "        predictions_total += predictions\n",
        "\n",
        "    # Evaluate BLEU score of the generated captions\n",
        "    bleu_1 = corpus_bleu(references_total, predictions_total, weights=bleu_w[\"bleu-1\"]) * 100\n",
        "    bleu_2 = corpus_bleu(references_total, predictions_total, weights=bleu_w[\"bleu-2\"]) * 100\n",
        "    bleu_3 = corpus_bleu(references_total, predictions_total, weights=bleu_w[\"bleu-3\"]) * 100\n",
        "    bleu_4 = corpus_bleu(references_total, predictions_total, weights=bleu_w[\"bleu-4\"]) * 100\n",
        "    bleu = [bleu_1, bleu_2, bleu_3, bleu_4]\n",
        "    return bleu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_DZ7Dgafflg",
        "outputId": "f6db2394-6d8c-47fa-b04d-a7cc7a1a43b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n",
            "Number of batches of 32 on training set:  1093\n",
            "Batch Step:  50\n",
            "Batch Loss:  tensor(5.7370, device='cuda:0')\n",
            "Batch Step:  100\n",
            "Batch Loss:  tensor(5.7799, device='cuda:0')\n",
            "Batch Step:  150\n",
            "Batch Loss:  tensor(5.8365, device='cuda:0')\n",
            "Batch Step:  200\n",
            "Batch Loss:  tensor(5.6079, device='cuda:0')\n",
            "Batch Step:  250\n",
            "Batch Loss:  tensor(5.7523, device='cuda:0')\n",
            "Batch Step:  300\n",
            "Batch Loss:  tensor(5.9917, device='cuda:0')\n",
            "Batch Step:  350\n",
            "Batch Loss:  tensor(5.8114, device='cuda:0')\n",
            "Batch Step:  400\n",
            "Batch Loss:  tensor(5.5458, device='cuda:0')\n",
            "Batch Step:  450\n",
            "Batch Loss:  tensor(5.7606, device='cuda:0')\n",
            "Batch Step:  500\n",
            "Batch Loss:  tensor(5.7411, device='cuda:0')\n",
            "Batch Step:  550\n",
            "Batch Loss:  tensor(5.7722, device='cuda:0')\n",
            "Batch Step:  600\n",
            "Batch Loss:  tensor(5.4226, device='cuda:0')\n",
            "Batch Step:  650\n",
            "Batch Loss:  tensor(5.7959, device='cuda:0')\n",
            "Batch Step:  700\n",
            "Batch Loss:  tensor(5.7883, device='cuda:0')\n",
            "Batch Step:  750\n",
            "Batch Loss:  tensor(5.7276, device='cuda:0')\n",
            "Batch Step:  800\n",
            "Batch Loss:  tensor(5.7096, device='cuda:0')\n",
            "Batch Step:  850\n",
            "Batch Loss:  tensor(5.4344, device='cuda:0')\n",
            "Batch Step:  900\n",
            "Batch Loss:  tensor(5.7968, device='cuda:0')\n",
            "Batch Step:  950\n",
            "Batch Loss:  tensor(5.7609, device='cuda:0')\n",
            "Batch Step:  1000\n",
            "Batch Loss:  tensor(5.6926, device='cuda:0')\n",
            "Batch Step:  1050\n",
            "Batch Loss:  tensor(5.7644, device='cuda:0')\n",
            "Epoch mean loss:  tensor(5.7058, device='cuda:0')\n",
            "Model saved.\n",
            "Evaluating model.\n",
            "[15.36690359395335, 6.95708063589308, 2.2875395869878994, 0.5131631490002007]\n",
            "\n",
            "Epoch: 1\n",
            "Number of batches of 32 on training set:  1093\n",
            "Batch Step:  50\n",
            "Batch Loss:  tensor(5.7496, device='cuda:0')\n",
            "Batch Step:  100\n",
            "Batch Loss:  tensor(5.6495, device='cuda:0')\n",
            "Batch Step:  150\n",
            "Batch Loss:  tensor(5.4401, device='cuda:0')\n",
            "Batch Step:  200\n",
            "Batch Loss:  tensor(5.8410, device='cuda:0')\n",
            "Batch Step:  250\n",
            "Batch Loss:  tensor(5.4645, device='cuda:0')\n",
            "Batch Step:  300\n",
            "Batch Loss:  tensor(5.5914, device='cuda:0')\n",
            "Batch Step:  350\n",
            "Batch Loss:  tensor(5.8050, device='cuda:0')\n",
            "Batch Step:  400\n",
            "Batch Loss:  tensor(5.3210, device='cuda:0')\n",
            "Batch Step:  450\n",
            "Batch Loss:  tensor(5.5022, device='cuda:0')\n",
            "Batch Step:  500\n",
            "Batch Loss:  tensor(5.3599, device='cuda:0')\n",
            "Batch Step:  550\n",
            "Batch Loss:  tensor(5.5706, device='cuda:0')\n",
            "Batch Step:  600\n",
            "Batch Loss:  tensor(5.6013, device='cuda:0')\n",
            "Batch Step:  650\n",
            "Batch Loss:  tensor(5.5371, device='cuda:0')\n",
            "Batch Step:  700\n",
            "Batch Loss:  tensor(5.3787, device='cuda:0')\n",
            "Batch Step:  750\n",
            "Batch Loss:  tensor(5.5859, device='cuda:0')\n",
            "Batch Step:  800\n",
            "Batch Loss:  tensor(5.6087, device='cuda:0')\n",
            "Batch Step:  850\n",
            "Batch Loss:  tensor(5.3946, device='cuda:0')\n",
            "Batch Step:  900\n",
            "Batch Loss:  tensor(5.3081, device='cuda:0')\n",
            "Batch Step:  950\n",
            "Batch Loss:  tensor(5.1797, device='cuda:0')\n",
            "Batch Step:  1000\n",
            "Batch Loss:  tensor(5.5335, device='cuda:0')\n",
            "Batch Step:  1050\n",
            "Batch Loss:  tensor(5.4510, device='cuda:0')\n",
            "Epoch mean loss:  tensor(5.5213, device='cuda:0')\n",
            "Model saved.\n",
            "Evaluating model.\n",
            "[15.617645800350862, 7.490580415244606, 2.501388315739692, 0.8822699377677866]\n",
            "\n",
            "Epoch: 2\n",
            "Number of batches of 32 on training set:  1093\n",
            "Batch Step:  50\n",
            "Batch Loss:  tensor(5.3957, device='cuda:0')\n",
            "Batch Step:  100\n",
            "Batch Loss:  tensor(5.2595, device='cuda:0')\n",
            "Batch Step:  150\n",
            "Batch Loss:  tensor(5.2965, device='cuda:0')\n",
            "Batch Step:  200\n",
            "Batch Loss:  tensor(5.4968, device='cuda:0')\n",
            "Batch Step:  250\n",
            "Batch Loss:  tensor(5.4883, device='cuda:0')\n",
            "Batch Step:  300\n",
            "Batch Loss:  tensor(5.1206, device='cuda:0')\n",
            "Batch Step:  350\n",
            "Batch Loss:  tensor(5.4299, device='cuda:0')\n",
            "Batch Step:  400\n",
            "Batch Loss:  tensor(5.3128, device='cuda:0')\n",
            "Batch Step:  450\n",
            "Batch Loss:  tensor(5.3390, device='cuda:0')\n",
            "Batch Step:  500\n",
            "Batch Loss:  tensor(5.4694, device='cuda:0')\n",
            "Batch Step:  550\n",
            "Batch Loss:  tensor(5.5219, device='cuda:0')\n",
            "Batch Step:  600\n",
            "Batch Loss:  tensor(5.2881, device='cuda:0')\n",
            "Batch Step:  650\n",
            "Batch Loss:  tensor(5.3676, device='cuda:0')\n",
            "Batch Step:  700\n",
            "Batch Loss:  tensor(5.2034, device='cuda:0')\n",
            "Batch Step:  750\n",
            "Batch Loss:  tensor(5.3790, device='cuda:0')\n",
            "Batch Step:  800\n",
            "Batch Loss:  tensor(5.2774, device='cuda:0')\n",
            "Batch Step:  850\n",
            "Batch Loss:  tensor(5.4567, device='cuda:0')\n",
            "Batch Step:  900\n",
            "Batch Loss:  tensor(5.4828, device='cuda:0')\n",
            "Batch Step:  950\n",
            "Batch Loss:  tensor(5.5741, device='cuda:0')\n",
            "Batch Step:  1000\n",
            "Batch Loss:  tensor(5.5570, device='cuda:0')\n",
            "Batch Step:  1050\n",
            "Batch Loss:  tensor(5.3319, device='cuda:0')\n",
            "Epoch mean loss:  tensor(5.3775, device='cuda:0')\n",
            "Model saved.\n",
            "Evaluating model.\n",
            "[18.568221745071245, 9.541063744459338, 3.688245099926752, 1.458708980404834]\n",
            "\n",
            "Epoch: 3\n",
            "Number of batches of 32 on training set:  1093\n",
            "Batch Step:  50\n",
            "Batch Loss:  tensor(5.1954, device='cuda:0')\n",
            "Batch Step:  100\n",
            "Batch Loss:  tensor(5.2449, device='cuda:0')\n",
            "Batch Step:  150\n",
            "Batch Loss:  tensor(5.2493, device='cuda:0')\n",
            "Batch Step:  200\n",
            "Batch Loss:  tensor(4.9680, device='cuda:0')\n",
            "Batch Step:  250\n",
            "Batch Loss:  tensor(5.3009, device='cuda:0')\n",
            "Batch Step:  300\n",
            "Batch Loss:  tensor(5.2404, device='cuda:0')\n",
            "Batch Step:  350\n",
            "Batch Loss:  tensor(5.2111, device='cuda:0')\n",
            "Batch Step:  400\n",
            "Batch Loss:  tensor(5.4929, device='cuda:0')\n",
            "Batch Step:  450\n",
            "Batch Loss:  tensor(5.3972, device='cuda:0')\n",
            "Batch Step:  500\n",
            "Batch Loss:  tensor(5.2936, device='cuda:0')\n",
            "Batch Step:  550\n",
            "Batch Loss:  tensor(5.2456, device='cuda:0')\n",
            "Batch Step:  600\n",
            "Batch Loss:  tensor(5.3295, device='cuda:0')\n",
            "Batch Step:  650\n",
            "Batch Loss:  tensor(5.2905, device='cuda:0')\n",
            "Batch Step:  700\n",
            "Batch Loss:  tensor(5.2764, device='cuda:0')\n",
            "Batch Step:  750\n",
            "Batch Loss:  tensor(5.0888, device='cuda:0')\n",
            "Batch Step:  800\n",
            "Batch Loss:  tensor(5.3894, device='cuda:0')\n",
            "Batch Step:  850\n",
            "Batch Loss:  tensor(5.1885, device='cuda:0')\n",
            "Batch Step:  900\n",
            "Batch Loss:  tensor(5.6640, device='cuda:0')\n",
            "Batch Step:  950\n",
            "Batch Loss:  tensor(5.2973, device='cuda:0')\n",
            "Batch Step:  1000\n",
            "Batch Loss:  tensor(5.0506, device='cuda:0')\n",
            "Batch Step:  1050\n",
            "Batch Loss:  tensor(5.2986, device='cuda:0')\n",
            "Epoch mean loss:  tensor(5.2665, device='cuda:0')\n",
            "Model saved.\n",
            "Evaluating model.\n",
            "[17.194912052390748, 9.110607410180096, 3.728279029315731, 1.574612374272535]\n",
            "\n",
            "Epoch: 4\n",
            "Number of batches of 32 on training set:  1093\n",
            "Batch Step:  50\n",
            "Batch Loss:  tensor(5.1482, device='cuda:0')\n",
            "Batch Step:  100\n",
            "Batch Loss:  tensor(5.2453, device='cuda:0')\n",
            "Batch Step:  150\n",
            "Batch Loss:  tensor(5.1184, device='cuda:0')\n",
            "Batch Step:  200\n",
            "Batch Loss:  tensor(4.9774, device='cuda:0')\n",
            "Batch Step:  250\n",
            "Batch Loss:  tensor(5.1648, device='cuda:0')\n",
            "Batch Step:  300\n",
            "Batch Loss:  tensor(5.2864, device='cuda:0')\n",
            "Batch Step:  350\n",
            "Batch Loss:  tensor(5.0263, device='cuda:0')\n",
            "Batch Step:  400\n",
            "Batch Loss:  tensor(4.9633, device='cuda:0')\n",
            "Batch Step:  450\n",
            "Batch Loss:  tensor(5.0940, device='cuda:0')\n",
            "Batch Step:  500\n",
            "Batch Loss:  tensor(5.2119, device='cuda:0')\n",
            "Batch Step:  550\n",
            "Batch Loss:  tensor(5.5141, device='cuda:0')\n",
            "Batch Step:  600\n",
            "Batch Loss:  tensor(5.0900, device='cuda:0')\n",
            "Batch Step:  650\n",
            "Batch Loss:  tensor(5.2093, device='cuda:0')\n",
            "Batch Step:  700\n",
            "Batch Loss:  tensor(5.1893, device='cuda:0')\n",
            "Batch Step:  750\n",
            "Batch Loss:  tensor(4.9710, device='cuda:0')\n",
            "Batch Step:  800\n",
            "Batch Loss:  tensor(5.2467, device='cuda:0')\n",
            "Batch Step:  850\n",
            "Batch Loss:  tensor(5.2330, device='cuda:0')\n",
            "Batch Step:  900\n",
            "Batch Loss:  tensor(5.1865, device='cuda:0')\n",
            "Batch Step:  950\n",
            "Batch Loss:  tensor(5.0288, device='cuda:0')\n",
            "Batch Step:  1000\n",
            "Batch Loss:  tensor(5.0254, device='cuda:0')\n",
            "Batch Step:  1050\n",
            "Batch Loss:  tensor(5.1989, device='cuda:0')\n",
            "Epoch mean loss:  tensor(5.1740, device='cuda:0')\n",
            "Model saved.\n",
            "Evaluating model.\n",
            "[20.32291865117165, 11.264601578698715, 5.142904895765203, 2.293328263637828]\n",
            "\n",
            "Epoch: 5\n",
            "Number of batches of 32 on training set:  1093\n",
            "Batch Step:  50\n",
            "Batch Loss:  tensor(4.9884, device='cuda:0')\n",
            "Batch Step:  100\n",
            "Batch Loss:  tensor(5.2836, device='cuda:0')\n",
            "Batch Step:  150\n",
            "Batch Loss:  tensor(5.1522, device='cuda:0')\n",
            "Batch Step:  200\n",
            "Batch Loss:  tensor(5.0635, device='cuda:0')\n",
            "Batch Step:  250\n",
            "Batch Loss:  tensor(4.9028, device='cuda:0')\n",
            "Batch Step:  300\n",
            "Batch Loss:  tensor(5.2497, device='cuda:0')\n",
            "Batch Step:  350\n",
            "Batch Loss:  tensor(5.1639, device='cuda:0')\n",
            "Batch Step:  400\n",
            "Batch Loss:  tensor(5.1569, device='cuda:0')\n",
            "Batch Step:  450\n",
            "Batch Loss:  tensor(4.8688, device='cuda:0')\n",
            "Batch Step:  500\n",
            "Batch Loss:  tensor(5.3566, device='cuda:0')\n",
            "Batch Step:  550\n",
            "Batch Loss:  tensor(5.1372, device='cuda:0')\n",
            "Batch Step:  600\n",
            "Batch Loss:  tensor(5.2514, device='cuda:0')\n",
            "Batch Step:  650\n",
            "Batch Loss:  tensor(5.3708, device='cuda:0')\n",
            "Batch Step:  700\n",
            "Batch Loss:  tensor(5.3195, device='cuda:0')\n",
            "Batch Step:  750\n",
            "Batch Loss:  tensor(5.1124, device='cuda:0')\n",
            "Batch Step:  800\n",
            "Batch Loss:  tensor(5.2978, device='cuda:0')\n",
            "Batch Step:  850\n",
            "Batch Loss:  tensor(5.2809, device='cuda:0')\n",
            "Batch Step:  900\n",
            "Batch Loss:  tensor(5.3997, device='cuda:0')\n",
            "Batch Step:  950\n",
            "Batch Loss:  tensor(5.0895, device='cuda:0')\n",
            "Batch Step:  1000\n",
            "Batch Loss:  tensor(4.9063, device='cuda:0')\n",
            "Batch Step:  1050\n",
            "Batch Loss:  tensor(5.0347, device='cuda:0')\n",
            "Epoch mean loss:  tensor(5.0938, device='cuda:0')\n",
            "Model saved.\n",
            "Evaluating model.\n",
            "[19.349104334746343, 10.788344084255922, 5.327683206739716, 2.5673966414609546]\n",
            "\n",
            "Epoch: 6\n",
            "Number of batches of 32 on training set:  1093\n",
            "Batch Step:  50\n",
            "Batch Loss:  tensor(5.1273, device='cuda:0')\n",
            "Batch Step:  100\n",
            "Batch Loss:  tensor(4.9569, device='cuda:0')\n",
            "Batch Step:  150\n",
            "Batch Loss:  tensor(5.0666, device='cuda:0')\n",
            "Batch Step:  200\n",
            "Batch Loss:  tensor(4.9781, device='cuda:0')\n",
            "Batch Step:  250\n",
            "Batch Loss:  tensor(4.9718, device='cuda:0')\n",
            "Batch Step:  300\n",
            "Batch Loss:  tensor(4.9217, device='cuda:0')\n",
            "Batch Step:  350\n",
            "Batch Loss:  tensor(5.1778, device='cuda:0')\n",
            "Batch Step:  400\n",
            "Batch Loss:  tensor(5.3310, device='cuda:0')\n",
            "Batch Step:  450\n",
            "Batch Loss:  tensor(5.0845, device='cuda:0')\n",
            "Batch Step:  500\n",
            "Batch Loss:  tensor(4.9971, device='cuda:0')\n",
            "Batch Step:  550\n",
            "Batch Loss:  tensor(5.3832, device='cuda:0')\n",
            "Batch Step:  600\n",
            "Batch Loss:  tensor(5.1716, device='cuda:0')\n",
            "Batch Step:  650\n",
            "Batch Loss:  tensor(4.9518, device='cuda:0')\n",
            "Batch Step:  700\n",
            "Batch Loss:  tensor(4.9439, device='cuda:0')\n",
            "Batch Step:  750\n",
            "Batch Loss:  tensor(4.8827, device='cuda:0')\n",
            "Batch Step:  800\n",
            "Batch Loss:  tensor(4.9045, device='cuda:0')\n",
            "Batch Step:  850\n",
            "Batch Loss:  tensor(4.9456, device='cuda:0')\n",
            "Batch Step:  900\n",
            "Batch Loss:  tensor(5.0675, device='cuda:0')\n",
            "Batch Step:  950\n",
            "Batch Loss:  tensor(4.7872, device='cuda:0')\n",
            "Batch Step:  1000\n",
            "Batch Loss:  tensor(5.2897, device='cuda:0')\n",
            "Batch Step:  1050\n",
            "Batch Loss:  tensor(4.8900, device='cuda:0')\n",
            "Epoch mean loss:  tensor(5.0249, device='cuda:0')\n",
            "Model saved.\n",
            "Evaluating model.\n",
            "[20.134070879429963, 11.517088961353785, 5.791934512781912, 2.8513463627491986]\n",
            "\n",
            "Epoch: 7\n",
            "Number of batches of 32 on training set:  1093\n",
            "Batch Step:  50\n",
            "Batch Loss:  tensor(5.0677, device='cuda:0')\n",
            "Batch Step:  100\n",
            "Batch Loss:  tensor(5.2100, device='cuda:0')\n",
            "Batch Step:  150\n",
            "Batch Loss:  tensor(4.7715, device='cuda:0')\n",
            "Batch Step:  200\n",
            "Batch Loss:  tensor(5.2326, device='cuda:0')\n",
            "Batch Step:  250\n",
            "Batch Loss:  tensor(4.8976, device='cuda:0')\n",
            "Batch Step:  300\n",
            "Batch Loss:  tensor(4.8731, device='cuda:0')\n",
            "Batch Step:  350\n",
            "Batch Loss:  tensor(4.8971, device='cuda:0')\n",
            "Batch Step:  400\n",
            "Batch Loss:  tensor(5.1803, device='cuda:0')\n",
            "Batch Step:  450\n",
            "Batch Loss:  tensor(4.7776, device='cuda:0')\n",
            "Batch Step:  500\n",
            "Batch Loss:  tensor(4.8588, device='cuda:0')\n",
            "Batch Step:  550\n",
            "Batch Loss:  tensor(4.8518, device='cuda:0')\n",
            "Batch Step:  600\n",
            "Batch Loss:  tensor(4.7162, device='cuda:0')\n",
            "Batch Step:  650\n",
            "Batch Loss:  tensor(4.9721, device='cuda:0')\n",
            "Batch Step:  700\n",
            "Batch Loss:  tensor(4.9900, device='cuda:0')\n",
            "Batch Step:  750\n",
            "Batch Loss:  tensor(4.7791, device='cuda:0')\n",
            "Batch Step:  800\n",
            "Batch Loss:  tensor(5.1015, device='cuda:0')\n",
            "Batch Step:  850\n",
            "Batch Loss:  tensor(5.2883, device='cuda:0')\n",
            "Batch Step:  900\n",
            "Batch Loss:  tensor(4.9758, device='cuda:0')\n",
            "Batch Step:  950\n",
            "Batch Loss:  tensor(4.9418, device='cuda:0')\n",
            "Batch Step:  1000\n",
            "Batch Loss:  tensor(4.9888, device='cuda:0')\n",
            "Batch Step:  1050\n",
            "Batch Loss:  tensor(4.9321, device='cuda:0')\n",
            "Epoch mean loss:  tensor(4.9617, device='cuda:0')\n",
            "Model saved.\n",
            "Evaluating model.\n",
            "[23.870747525315736, 13.975009217278394, 7.329985625284727, 3.659115027572547]\n",
            "\n",
            "Epoch: 8\n",
            "Number of batches of 32 on training set:  1093\n",
            "Batch Step:  50\n",
            "Batch Loss:  tensor(4.8542, device='cuda:0')\n",
            "Batch Step:  100\n",
            "Batch Loss:  tensor(4.5170, device='cuda:0')\n",
            "Batch Step:  150\n",
            "Batch Loss:  tensor(5.0751, device='cuda:0')\n",
            "Batch Step:  200\n",
            "Batch Loss:  tensor(4.7847, device='cuda:0')\n",
            "Batch Step:  250\n",
            "Batch Loss:  tensor(4.9952, device='cuda:0')\n",
            "Batch Step:  300\n",
            "Batch Loss:  tensor(4.7493, device='cuda:0')\n",
            "Batch Step:  350\n",
            "Batch Loss:  tensor(5.0489, device='cuda:0')\n",
            "Batch Step:  400\n",
            "Batch Loss:  tensor(4.8918, device='cuda:0')\n",
            "Batch Step:  450\n",
            "Batch Loss:  tensor(4.8168, device='cuda:0')\n",
            "Batch Step:  500\n",
            "Batch Loss:  tensor(4.9662, device='cuda:0')\n",
            "Batch Step:  550\n",
            "Batch Loss:  tensor(5.1965, device='cuda:0')\n",
            "Batch Step:  600\n",
            "Batch Loss:  tensor(5.1647, device='cuda:0')\n",
            "Batch Step:  650\n",
            "Batch Loss:  tensor(4.8720, device='cuda:0')\n",
            "Batch Step:  700\n",
            "Batch Loss:  tensor(5.0282, device='cuda:0')\n",
            "Batch Step:  750\n",
            "Batch Loss:  tensor(4.9905, device='cuda:0')\n",
            "Batch Step:  800\n",
            "Batch Loss:  tensor(4.4402, device='cuda:0')\n",
            "Batch Step:  850\n",
            "Batch Loss:  tensor(5.0148, device='cuda:0')\n",
            "Batch Step:  900\n",
            "Batch Loss:  tensor(4.6885, device='cuda:0')\n",
            "Batch Step:  950\n",
            "Batch Loss:  tensor(5.0118, device='cuda:0')\n",
            "Batch Step:  1000\n",
            "Batch Loss:  tensor(4.9180, device='cuda:0')\n",
            "Batch Step:  1050\n",
            "Batch Loss:  tensor(4.9097, device='cuda:0')\n",
            "Epoch mean loss:  tensor(4.9056, device='cuda:0')\n",
            "Model saved.\n",
            "Evaluating model.\n",
            "[29.56345035386448, 17.616769055330337, 9.375898721607165, 4.7111221469885205]\n",
            "\n",
            "Epoch: 9\n",
            "Number of batches of 32 on training set:  1093\n",
            "Batch Step:  50\n",
            "Batch Loss:  tensor(4.7783, device='cuda:0')\n",
            "Batch Step:  100\n",
            "Batch Loss:  tensor(4.7639, device='cuda:0')\n",
            "Batch Step:  150\n",
            "Batch Loss:  tensor(4.8257, device='cuda:0')\n",
            "Batch Step:  200\n",
            "Batch Loss:  tensor(5.0469, device='cuda:0')\n",
            "Batch Step:  250\n",
            "Batch Loss:  tensor(4.7372, device='cuda:0')\n",
            "Batch Step:  300\n",
            "Batch Loss:  tensor(5.0988, device='cuda:0')\n",
            "Batch Step:  350\n",
            "Batch Loss:  tensor(4.8230, device='cuda:0')\n",
            "Batch Step:  400\n",
            "Batch Loss:  tensor(4.7611, device='cuda:0')\n",
            "Batch Step:  450\n",
            "Batch Loss:  tensor(4.7446, device='cuda:0')\n",
            "Batch Step:  500\n",
            "Batch Loss:  tensor(4.6395, device='cuda:0')\n",
            "Batch Step:  550\n",
            "Batch Loss:  tensor(5.0671, device='cuda:0')\n",
            "Batch Step:  600\n",
            "Batch Loss:  tensor(5.0233, device='cuda:0')\n",
            "Batch Step:  650\n",
            "Batch Loss:  tensor(5.1124, device='cuda:0')\n",
            "Batch Step:  700\n",
            "Batch Loss:  tensor(4.8518, device='cuda:0')\n",
            "Batch Step:  750\n",
            "Batch Loss:  tensor(4.8006, device='cuda:0')\n",
            "Batch Step:  800\n",
            "Batch Loss:  tensor(4.7514, device='cuda:0')\n",
            "Batch Step:  850\n",
            "Batch Loss:  tensor(4.7901, device='cuda:0')\n",
            "Batch Step:  900\n",
            "Batch Loss:  tensor(4.4985, device='cuda:0')\n",
            "Batch Step:  950\n",
            "Batch Loss:  tensor(5.0290, device='cuda:0')\n",
            "Batch Step:  1000\n",
            "Batch Loss:  tensor(4.8596, device='cuda:0')\n",
            "Batch Step:  1050\n",
            "Batch Loss:  tensor(4.8808, device='cuda:0')\n",
            "Epoch mean loss:  tensor(4.8543, device='cuda:0')\n",
            "Model saved.\n",
            "Evaluating model.\n",
            "[30.070327484549264, 18.096294176939846, 9.947656124255143, 5.183600095669729]\n",
            "\n",
            "Epoch: 10\n",
            "Number of batches of 32 on training set:  1093\n",
            "Batch Step:  50\n",
            "Batch Loss:  tensor(4.8026, device='cuda:0')\n",
            "Batch Step:  100\n",
            "Batch Loss:  tensor(5.1545, device='cuda:0')\n",
            "Batch Step:  150\n",
            "Batch Loss:  tensor(4.7625, device='cuda:0')\n",
            "Batch Step:  200\n",
            "Batch Loss:  tensor(4.9498, device='cuda:0')\n",
            "Batch Step:  250\n",
            "Batch Loss:  tensor(4.9936, device='cuda:0')\n",
            "Batch Step:  300\n",
            "Batch Loss:  tensor(4.7444, device='cuda:0')\n",
            "Batch Step:  350\n",
            "Batch Loss:  tensor(4.8970, device='cuda:0')\n",
            "Batch Step:  400\n",
            "Batch Loss:  tensor(4.7630, device='cuda:0')\n",
            "Batch Step:  450\n",
            "Batch Loss:  tensor(4.8893, device='cuda:0')\n",
            "Batch Step:  500\n",
            "Batch Loss:  tensor(4.7400, device='cuda:0')\n",
            "Batch Step:  550\n",
            "Batch Loss:  tensor(4.6729, device='cuda:0')\n",
            "Batch Step:  600\n",
            "Batch Loss:  tensor(4.8358, device='cuda:0')\n",
            "Batch Step:  650\n",
            "Batch Loss:  tensor(5.1207, device='cuda:0')\n",
            "Batch Step:  700\n",
            "Batch Loss:  tensor(4.9199, device='cuda:0')\n",
            "Batch Step:  750\n",
            "Batch Loss:  tensor(4.7321, device='cuda:0')\n",
            "Batch Step:  800\n",
            "Batch Loss:  tensor(4.8123, device='cuda:0')\n",
            "Batch Step:  850\n",
            "Batch Loss:  tensor(4.8695, device='cuda:0')\n",
            "Batch Step:  900\n",
            "Batch Loss:  tensor(4.4352, device='cuda:0')\n",
            "Batch Step:  950\n",
            "Batch Loss:  tensor(4.7279, device='cuda:0')\n",
            "Batch Step:  1000\n",
            "Batch Loss:  tensor(5.0887, device='cuda:0')\n",
            "Batch Step:  1050\n",
            "Batch Loss:  tensor(4.6791, device='cuda:0')\n",
            "Epoch mean loss:  tensor(4.8058, device='cuda:0')\n",
            "Model saved.\n",
            "Evaluating model.\n",
            "[39.1116026172703, 23.992200316109173, 13.382918338792853, 7.057008329974623]\n",
            "\n",
            "Epoch: 11\n",
            "Number of batches of 32 on training set:  1093\n",
            "Batch Step:  50\n",
            "Batch Loss:  tensor(4.9232, device='cuda:0')\n",
            "Batch Step:  100\n",
            "Batch Loss:  tensor(4.7614, device='cuda:0')\n",
            "Batch Step:  150\n",
            "Batch Loss:  tensor(4.7274, device='cuda:0')\n",
            "Batch Step:  200\n",
            "Batch Loss:  tensor(4.6573, device='cuda:0')\n",
            "Batch Step:  250\n",
            "Batch Loss:  tensor(4.5679, device='cuda:0')\n",
            "Batch Step:  300\n",
            "Batch Loss:  tensor(4.6443, device='cuda:0')\n",
            "Batch Step:  350\n",
            "Batch Loss:  tensor(4.8345, device='cuda:0')\n",
            "Batch Step:  400\n",
            "Batch Loss:  tensor(4.5657, device='cuda:0')\n",
            "Batch Step:  450\n",
            "Batch Loss:  tensor(4.7867, device='cuda:0')\n",
            "Batch Step:  500\n",
            "Batch Loss:  tensor(4.9069, device='cuda:0')\n",
            "Batch Step:  550\n",
            "Batch Loss:  tensor(4.6958, device='cuda:0')\n",
            "Batch Step:  600\n",
            "Batch Loss:  tensor(4.7856, device='cuda:0')\n",
            "Batch Step:  650\n",
            "Batch Loss:  tensor(4.5796, device='cuda:0')\n",
            "Batch Step:  700\n",
            "Batch Loss:  tensor(4.5899, device='cuda:0')\n",
            "Batch Step:  750\n",
            "Batch Loss:  tensor(4.5814, device='cuda:0')\n",
            "Batch Step:  800\n",
            "Batch Loss:  tensor(4.6673, device='cuda:0')\n",
            "Batch Step:  850\n",
            "Batch Loss:  tensor(4.7294, device='cuda:0')\n",
            "Batch Step:  900\n",
            "Batch Loss:  tensor(4.9526, device='cuda:0')\n",
            "Batch Step:  950\n",
            "Batch Loss:  tensor(4.4938, device='cuda:0')\n",
            "Batch Step:  1000\n",
            "Batch Loss:  tensor(4.5670, device='cuda:0')\n",
            "Batch Step:  1050\n",
            "Batch Loss:  tensor(4.8581, device='cuda:0')\n",
            "Epoch mean loss:  tensor(4.7602, device='cuda:0')\n",
            "Model saved.\n",
            "Evaluating model.\n",
            "[45.7312466270912, 28.350270778366543, 16.199390743280766, 8.818485197251096]\n",
            "\n",
            "Epoch: 12\n",
            "Number of batches of 32 on training set:  1093\n",
            "Batch Step:  50\n",
            "Batch Loss:  tensor(4.9358, device='cuda:0')\n",
            "Batch Step:  100\n",
            "Batch Loss:  tensor(4.8841, device='cuda:0')\n",
            "Batch Step:  150\n",
            "Batch Loss:  tensor(4.8247, device='cuda:0')\n",
            "Batch Step:  200\n",
            "Batch Loss:  tensor(4.4926, device='cuda:0')\n",
            "Batch Step:  250\n",
            "Batch Loss:  tensor(4.7482, device='cuda:0')\n",
            "Batch Step:  300\n",
            "Batch Loss:  tensor(5.0453, device='cuda:0')\n",
            "Batch Step:  350\n",
            "Batch Loss:  tensor(4.5565, device='cuda:0')\n",
            "Batch Step:  400\n",
            "Batch Loss:  tensor(5.0489, device='cuda:0')\n",
            "Batch Step:  450\n",
            "Batch Loss:  tensor(4.5894, device='cuda:0')\n",
            "Batch Step:  500\n",
            "Batch Loss:  tensor(4.6503, device='cuda:0')\n",
            "Batch Step:  550\n",
            "Batch Loss:  tensor(4.8265, device='cuda:0')\n",
            "Batch Step:  600\n",
            "Batch Loss:  tensor(4.7546, device='cuda:0')\n",
            "Batch Step:  650\n",
            "Batch Loss:  tensor(4.7146, device='cuda:0')\n",
            "Batch Step:  700\n",
            "Batch Loss:  tensor(4.7150, device='cuda:0')\n",
            "Batch Step:  750\n",
            "Batch Loss:  tensor(4.6550, device='cuda:0')\n",
            "Batch Step:  800\n",
            "Batch Loss:  tensor(4.7248, device='cuda:0')\n",
            "Batch Step:  850\n",
            "Batch Loss:  tensor(4.9288, device='cuda:0')\n",
            "Batch Step:  900\n",
            "Batch Loss:  tensor(4.8288, device='cuda:0')\n",
            "Batch Step:  950\n",
            "Batch Loss:  tensor(4.9990, device='cuda:0')\n",
            "Batch Step:  1000\n",
            "Batch Loss:  tensor(4.7129, device='cuda:0')\n",
            "Batch Step:  1050\n",
            "Batch Loss:  tensor(4.7521, device='cuda:0')\n",
            "Epoch mean loss:  tensor(4.7199, device='cuda:0')\n",
            "Model saved.\n",
            "Evaluating model.\n",
            "None of the BLEU scores increased, patience down:  5\n",
            "[43.08558785074927, 26.753814841640132, 15.527949826282809, 8.549401272808138]\n",
            "\n",
            "Epoch: 13\n",
            "Number of batches of 32 on training set:  1093\n",
            "Batch Step:  50\n",
            "Batch Loss:  tensor(4.7267, device='cuda:0')\n",
            "Batch Step:  100\n",
            "Batch Loss:  tensor(4.7734, device='cuda:0')\n",
            "Batch Step:  150\n",
            "Batch Loss:  tensor(4.6053, device='cuda:0')\n",
            "Batch Step:  200\n",
            "Batch Loss:  tensor(4.6446, device='cuda:0')\n",
            "Batch Step:  250\n",
            "Batch Loss:  tensor(5.0038, device='cuda:0')\n",
            "Batch Step:  300\n",
            "Batch Loss:  tensor(4.8368, device='cuda:0')\n",
            "Batch Step:  350\n",
            "Batch Loss:  tensor(4.5258, device='cuda:0')\n",
            "Batch Step:  400\n",
            "Batch Loss:  tensor(4.5222, device='cuda:0')\n",
            "Batch Step:  450\n",
            "Batch Loss:  tensor(4.8120, device='cuda:0')\n",
            "Batch Step:  500\n",
            "Batch Loss:  tensor(4.5396, device='cuda:0')\n",
            "Batch Step:  550\n",
            "Batch Loss:  tensor(4.7463, device='cuda:0')\n",
            "Batch Step:  600\n",
            "Batch Loss:  tensor(4.7203, device='cuda:0')\n",
            "Batch Step:  650\n",
            "Batch Loss:  tensor(4.6294, device='cuda:0')\n",
            "Batch Step:  700\n",
            "Batch Loss:  tensor(4.7258, device='cuda:0')\n",
            "Batch Step:  750\n",
            "Batch Loss:  tensor(4.2957, device='cuda:0')\n",
            "Batch Step:  800\n",
            "Batch Loss:  tensor(4.5127, device='cuda:0')\n",
            "Batch Step:  850\n",
            "Batch Loss:  tensor(4.9361, device='cuda:0')\n",
            "Batch Step:  900\n",
            "Batch Loss:  tensor(4.5466, device='cuda:0')\n",
            "Batch Step:  950\n",
            "Batch Loss:  tensor(4.5125, device='cuda:0')\n",
            "Batch Step:  1000\n",
            "Batch Loss:  tensor(4.5298, device='cuda:0')\n",
            "Batch Step:  1050\n",
            "Batch Loss:  tensor(4.6353, device='cuda:0')\n",
            "Epoch mean loss:  tensor(4.6798, device='cuda:0')\n",
            "Model saved.\n",
            "Evaluating model.\n",
            "[45.00568123127776, 28.040429213768153, 16.432196295631485, 9.095474092438712]\n",
            "\n",
            "Epoch: 14\n",
            "Number of batches of 32 on training set:  1093\n",
            "Batch Step:  50\n",
            "Batch Loss:  tensor(4.4707, device='cuda:0')\n",
            "Batch Step:  100\n",
            "Batch Loss:  tensor(4.5190, device='cuda:0')\n",
            "Batch Step:  150\n",
            "Batch Loss:  tensor(4.5048, device='cuda:0')\n",
            "Batch Step:  200\n",
            "Batch Loss:  tensor(4.4822, device='cuda:0')\n",
            "Batch Step:  250\n",
            "Batch Loss:  tensor(4.4253, device='cuda:0')\n",
            "Batch Step:  300\n",
            "Batch Loss:  tensor(4.8421, device='cuda:0')\n",
            "Batch Step:  350\n",
            "Batch Loss:  tensor(4.4466, device='cuda:0')\n",
            "Batch Step:  400\n",
            "Batch Loss:  tensor(4.5923, device='cuda:0')\n",
            "Batch Step:  450\n",
            "Batch Loss:  tensor(4.2748, device='cuda:0')\n",
            "Batch Step:  500\n",
            "Batch Loss:  tensor(4.5346, device='cuda:0')\n",
            "Batch Step:  550\n",
            "Batch Loss:  tensor(4.2923, device='cuda:0')\n",
            "Batch Step:  600\n",
            "Batch Loss:  tensor(4.8543, device='cuda:0')\n",
            "Batch Step:  650\n",
            "Batch Loss:  tensor(4.5300, device='cuda:0')\n",
            "Batch Step:  700\n",
            "Batch Loss:  tensor(4.3939, device='cuda:0')\n",
            "Batch Step:  750\n",
            "Batch Loss:  tensor(4.8234, device='cuda:0')\n",
            "Batch Step:  800\n",
            "Batch Loss:  tensor(4.6893, device='cuda:0')\n",
            "Batch Step:  850\n",
            "Batch Loss:  tensor(4.6450, device='cuda:0')\n",
            "Batch Step:  900\n",
            "Batch Loss:  tensor(4.8338, device='cuda:0')\n",
            "Batch Step:  950\n",
            "Batch Loss:  tensor(4.4615, device='cuda:0')\n",
            "Batch Step:  1000\n",
            "Batch Loss:  tensor(4.6492, device='cuda:0')\n",
            "Batch Step:  1050\n",
            "Batch Loss:  tensor(4.8115, device='cuda:0')\n",
            "Epoch mean loss:  tensor(4.6432, device='cuda:0')\n",
            "Model saved.\n",
            "Evaluating model.\n",
            "[46.00649010781953, 28.805203670778816, 17.002639181409805, 9.399395585584278]\n",
            "\n",
            "Epoch: 15\n",
            "Number of batches of 32 on training set:  1093\n",
            "Batch Step:  50\n",
            "Batch Loss:  tensor(4.6065, device='cuda:0')\n",
            "Batch Step:  100\n",
            "Batch Loss:  tensor(4.5746, device='cuda:0')\n",
            "Batch Step:  150\n",
            "Batch Loss:  tensor(4.5299, device='cuda:0')\n",
            "Batch Step:  200\n",
            "Batch Loss:  tensor(4.7326, device='cuda:0')\n",
            "Batch Step:  250\n",
            "Batch Loss:  tensor(4.4826, device='cuda:0')\n",
            "Batch Step:  300\n",
            "Batch Loss:  tensor(4.6122, device='cuda:0')\n",
            "Batch Step:  350\n",
            "Batch Loss:  tensor(4.6021, device='cuda:0')\n",
            "Batch Step:  400\n",
            "Batch Loss:  tensor(4.6844, device='cuda:0')\n",
            "Batch Step:  450\n",
            "Batch Loss:  tensor(4.6454, device='cuda:0')\n",
            "Batch Step:  500\n",
            "Batch Loss:  tensor(4.6736, device='cuda:0')\n",
            "Batch Step:  550\n",
            "Batch Loss:  tensor(4.4675, device='cuda:0')\n",
            "Batch Step:  600\n",
            "Batch Loss:  tensor(4.7140, device='cuda:0')\n",
            "Batch Step:  650\n",
            "Batch Loss:  tensor(4.7958, device='cuda:0')\n",
            "Batch Step:  700\n",
            "Batch Loss:  tensor(4.6418, device='cuda:0')\n",
            "Batch Step:  750\n",
            "Batch Loss:  tensor(4.7307, device='cuda:0')\n",
            "Batch Step:  800\n",
            "Batch Loss:  tensor(4.5665, device='cuda:0')\n",
            "Batch Step:  850\n",
            "Batch Loss:  tensor(4.6018, device='cuda:0')\n",
            "Batch Step:  900\n",
            "Batch Loss:  tensor(4.4036, device='cuda:0')\n",
            "Batch Step:  950\n",
            "Batch Loss:  tensor(4.4738, device='cuda:0')\n",
            "Batch Step:  1000\n",
            "Batch Loss:  tensor(4.5914, device='cuda:0')\n",
            "Batch Step:  1050\n",
            "Batch Loss:  tensor(4.4861, device='cuda:0')\n",
            "Epoch mean loss:  tensor(4.6098, device='cuda:0')\n",
            "Model saved.\n",
            "Evaluating model.\n",
            "None of the BLEU scores increased, patience down:  4\n",
            "[45.43748044634477, 28.315644272434376, 16.757996812493488, 9.393613828882167]\n",
            "\n",
            "Epoch: 16\n",
            "Number of batches of 32 on training set:  1093\n",
            "Batch Step:  50\n",
            "Batch Loss:  tensor(4.7107, device='cuda:0')\n",
            "Batch Step:  100\n",
            "Batch Loss:  tensor(4.3372, device='cuda:0')\n",
            "Batch Step:  150\n",
            "Batch Loss:  tensor(4.7045, device='cuda:0')\n",
            "Batch Step:  200\n",
            "Batch Loss:  tensor(4.5443, device='cuda:0')\n",
            "Batch Step:  250\n",
            "Batch Loss:  tensor(4.6808, device='cuda:0')\n",
            "Batch Step:  300\n",
            "Batch Loss:  tensor(4.4702, device='cuda:0')\n",
            "Batch Step:  350\n",
            "Batch Loss:  tensor(4.7120, device='cuda:0')\n",
            "Batch Step:  400\n",
            "Batch Loss:  tensor(4.7649, device='cuda:0')\n",
            "Batch Step:  450\n",
            "Batch Loss:  tensor(4.7800, device='cuda:0')\n",
            "Batch Step:  500\n",
            "Batch Loss:  tensor(4.4171, device='cuda:0')\n",
            "Batch Step:  550\n",
            "Batch Loss:  tensor(4.6242, device='cuda:0')\n",
            "Batch Step:  600\n",
            "Batch Loss:  tensor(4.3460, device='cuda:0')\n",
            "Batch Step:  650\n",
            "Batch Loss:  tensor(4.3547, device='cuda:0')\n",
            "Batch Step:  700\n",
            "Batch Loss:  tensor(4.6622, device='cuda:0')\n",
            "Batch Step:  750\n",
            "Batch Loss:  tensor(4.6540, device='cuda:0')\n",
            "Batch Step:  800\n",
            "Batch Loss:  tensor(4.8372, device='cuda:0')\n",
            "Batch Step:  850\n",
            "Batch Loss:  tensor(4.2055, device='cuda:0')\n",
            "Batch Step:  900\n",
            "Batch Loss:  tensor(4.1182, device='cuda:0')\n",
            "Batch Step:  950\n",
            "Batch Loss:  tensor(4.4336, device='cuda:0')\n",
            "Batch Step:  1000\n",
            "Batch Loss:  tensor(4.5270, device='cuda:0')\n",
            "Batch Step:  1050\n",
            "Batch Loss:  tensor(4.6573, device='cuda:0')\n",
            "Epoch mean loss:  tensor(4.5776, device='cuda:0')\n",
            "Model saved.\n",
            "Evaluating model.\n",
            "[50.26136363636363, 31.78035199407001, 19.08775564849344, 10.971689634122693]\n",
            "\n",
            "Epoch: 17\n",
            "Number of batches of 32 on training set:  1093\n",
            "Batch Step:  50\n",
            "Batch Loss:  tensor(4.4407, device='cuda:0')\n",
            "Batch Step:  100\n",
            "Batch Loss:  tensor(4.3426, device='cuda:0')\n",
            "Batch Step:  150\n",
            "Batch Loss:  tensor(4.6825, device='cuda:0')\n",
            "Batch Step:  200\n",
            "Batch Loss:  tensor(4.4131, device='cuda:0')\n",
            "Batch Step:  250\n",
            "Batch Loss:  tensor(4.6359, device='cuda:0')\n",
            "Batch Step:  300\n",
            "Batch Loss:  tensor(4.2599, device='cuda:0')\n",
            "Batch Step:  350\n",
            "Batch Loss:  tensor(4.5067, device='cuda:0')\n",
            "Batch Step:  400\n",
            "Batch Loss:  tensor(4.9025, device='cuda:0')\n",
            "Batch Step:  450\n",
            "Batch Loss:  tensor(4.6230, device='cuda:0')\n",
            "Batch Step:  500\n",
            "Batch Loss:  tensor(4.6534, device='cuda:0')\n",
            "Batch Step:  550\n",
            "Batch Loss:  tensor(4.5087, device='cuda:0')\n",
            "Batch Step:  600\n",
            "Batch Loss:  tensor(4.1762, device='cuda:0')\n",
            "Batch Step:  650\n",
            "Batch Loss:  tensor(4.7869, device='cuda:0')\n",
            "Batch Step:  700\n",
            "Batch Loss:  tensor(4.6009, device='cuda:0')\n",
            "Batch Step:  750\n",
            "Batch Loss:  tensor(4.5586, device='cuda:0')\n",
            "Batch Step:  800\n",
            "Batch Loss:  tensor(4.5572, device='cuda:0')\n",
            "Batch Step:  850\n",
            "Batch Loss:  tensor(4.6421, device='cuda:0')\n",
            "Batch Step:  900\n",
            "Batch Loss:  tensor(4.7639, device='cuda:0')\n",
            "Batch Step:  950\n",
            "Batch Loss:  tensor(4.5512, device='cuda:0')\n",
            "Batch Step:  1000\n",
            "Batch Loss:  tensor(4.1032, device='cuda:0')\n",
            "Batch Step:  1050\n",
            "Batch Loss:  tensor(4.7374, device='cuda:0')\n",
            "Epoch mean loss:  tensor(4.5448, device='cuda:0')\n",
            "Model saved.\n",
            "Evaluating model.\n",
            "None of the BLEU scores increased, patience down:  3\n",
            "[49.78185479360107, 31.490125232256617, 18.89797014693764, 10.690057036783559]\n",
            "\n",
            "Epoch: 18\n",
            "Number of batches of 32 on training set:  1093\n",
            "Batch Step:  50\n",
            "Batch Loss:  tensor(4.5925, device='cuda:0')\n",
            "Batch Step:  100\n",
            "Batch Loss:  tensor(4.5835, device='cuda:0')\n",
            "Batch Step:  150\n",
            "Batch Loss:  tensor(4.4532, device='cuda:0')\n",
            "Batch Step:  200\n",
            "Batch Loss:  tensor(4.8938, device='cuda:0')\n",
            "Batch Step:  250\n",
            "Batch Loss:  tensor(4.4837, device='cuda:0')\n",
            "Batch Step:  300\n",
            "Batch Loss:  tensor(4.7499, device='cuda:0')\n",
            "Batch Step:  350\n",
            "Batch Loss:  tensor(4.2199, device='cuda:0')\n",
            "Batch Step:  400\n",
            "Batch Loss:  tensor(4.7717, device='cuda:0')\n",
            "Batch Step:  450\n",
            "Batch Loss:  tensor(4.3514, device='cuda:0')\n",
            "Batch Step:  500\n",
            "Batch Loss:  tensor(4.6908, device='cuda:0')\n",
            "Batch Step:  550\n",
            "Batch Loss:  tensor(4.5473, device='cuda:0')\n",
            "Batch Step:  600\n",
            "Batch Loss:  tensor(4.6879, device='cuda:0')\n",
            "Batch Step:  650\n",
            "Batch Loss:  tensor(4.6962, device='cuda:0')\n",
            "Batch Step:  700\n",
            "Batch Loss:  tensor(4.4372, device='cuda:0')\n",
            "Batch Step:  750\n",
            "Batch Loss:  tensor(4.3590, device='cuda:0')\n",
            "Batch Step:  800\n",
            "Batch Loss:  tensor(4.6686, device='cuda:0')\n",
            "Batch Step:  850\n",
            "Batch Loss:  tensor(4.6031, device='cuda:0')\n",
            "Batch Step:  900\n",
            "Batch Loss:  tensor(4.5940, device='cuda:0')\n",
            "Batch Step:  950\n",
            "Batch Loss:  tensor(4.6405, device='cuda:0')\n",
            "Batch Step:  1000\n",
            "Batch Loss:  tensor(4.4195, device='cuda:0')\n",
            "Batch Step:  1050\n",
            "Batch Loss:  tensor(4.6121, device='cuda:0')\n",
            "Epoch mean loss:  tensor(4.5170, device='cuda:0')\n",
            "Model saved.\n",
            "Evaluating model.\n",
            "None of the BLEU scores increased, patience down:  2\n",
            "[47.915103440883264, 30.4567164078847, 18.539243101155346, 10.712755651944263]\n",
            "\n",
            "Epoch: 19\n",
            "Number of batches of 32 on training set:  1093\n",
            "Batch Step:  50\n",
            "Batch Loss:  tensor(4.4667, device='cuda:0')\n",
            "Batch Step:  100\n",
            "Batch Loss:  tensor(4.5559, device='cuda:0')\n",
            "Batch Step:  150\n",
            "Batch Loss:  tensor(4.4531, device='cuda:0')\n",
            "Batch Step:  200\n",
            "Batch Loss:  tensor(4.3662, device='cuda:0')\n",
            "Batch Step:  250\n",
            "Batch Loss:  tensor(4.5375, device='cuda:0')\n",
            "Batch Step:  300\n",
            "Batch Loss:  tensor(4.3436, device='cuda:0')\n",
            "Batch Step:  350\n",
            "Batch Loss:  tensor(4.5814, device='cuda:0')\n",
            "Batch Step:  400\n",
            "Batch Loss:  tensor(4.4673, device='cuda:0')\n",
            "Batch Step:  450\n",
            "Batch Loss:  tensor(4.6063, device='cuda:0')\n",
            "Batch Step:  500\n",
            "Batch Loss:  tensor(4.4261, device='cuda:0')\n",
            "Batch Step:  550\n",
            "Batch Loss:  tensor(4.2175, device='cuda:0')\n",
            "Batch Step:  600\n",
            "Batch Loss:  tensor(4.2988, device='cuda:0')\n",
            "Batch Step:  650\n",
            "Batch Loss:  tensor(4.6868, device='cuda:0')\n",
            "Batch Step:  700\n",
            "Batch Loss:  tensor(4.2830, device='cuda:0')\n",
            "Batch Step:  750\n",
            "Batch Loss:  tensor(4.4151, device='cuda:0')\n",
            "Batch Step:  800\n",
            "Batch Loss:  tensor(4.5137, device='cuda:0')\n",
            "Batch Step:  850\n",
            "Batch Loss:  tensor(4.3174, device='cuda:0')\n",
            "Batch Step:  900\n",
            "Batch Loss:  tensor(4.5343, device='cuda:0')\n",
            "Batch Step:  950\n",
            "Batch Loss:  tensor(4.3656, device='cuda:0')\n",
            "Batch Step:  1000\n",
            "Batch Loss:  tensor(4.4457, device='cuda:0')\n",
            "Batch Step:  1050\n",
            "Batch Loss:  tensor(4.4983, device='cuda:0')\n",
            "Epoch mean loss:  tensor(4.4879, device='cuda:0')\n",
            "Model saved.\n",
            "Evaluating model.\n",
            "None of the BLEU scores increased, patience down:  1\n",
            "[48.57734656055816, 30.668411836299285, 18.683627072073214, 10.738335752337747]\n",
            "Training stopped due to early stopping.\n"
          ]
        }
      ],
      "source": [
        "start_time = time.strftime(\"%b-%d_%H-%M-%S\")\n",
        "\n",
        "max_bleu = [0.0, 0.0, 0.0, 0.0]\n",
        "patience = 5\n",
        "\n",
        "for epoch in range(NUM_OF_EPOCHS):\n",
        "    print(\"Epoch:\", epoch)\n",
        "    encoder_decoder.set_encoder_train_mode(False)\n",
        "    encoder_decoder.set_decoder_train_mode(True)\n",
        "\n",
        "    print(\"Number of batches of 32 on training set: \", len(train_set)//BATCH_SIZE)\n",
        "    batch_step = 0\n",
        "    batch_loss = 0\n",
        "    for x_img, x_words, y, tgt_padding_mask in train_loader:\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        batch_step += 1\n",
        "\n",
        "        x_img, x_words = x_img.to(device), x_words.to(device)\n",
        "        y = y.to(device)\n",
        "        tgt_padding_mask = tgt_padding_mask.to(device)\n",
        "\n",
        "\n",
        "        y_pred = encoder_decoder(x_img, x_words, tgt_padding_mask)\n",
        "        tgt_padding_mask = torch.logical_not(tgt_padding_mask)\n",
        "        y_pred = y_pred[tgt_padding_mask]\n",
        "\n",
        "        y = y[tgt_padding_mask]\n",
        "\n",
        "        loss = loss_fcn(y_pred, y.long())\n",
        "        batch_loss += loss.detach()\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(encoder_decoder.parameters(), GRADIENT_CLIPPING)\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_step % 50 == 0:\n",
        "            print(\"Batch Step: \", batch_step)\n",
        "            print(\"Batch Loss: \", loss.detach())\n",
        "\n",
        "    print(\"Epoch mean loss: \", batch_loss/batch_step)\n",
        "\n",
        "    save_checkpoint(encoder_decoder, optimizer, start_time, epoch)\n",
        "    if (epoch + 1) % EVAL_PERIOD == 0:\n",
        "        with torch.no_grad():\n",
        "            encoder_decoder.set_decoder_train_mode(False)\n",
        "\n",
        "            valid_bleu = evaluate(valid_set, encoder_decoder, device)\n",
        "\n",
        "            if any(x > y for x, y in zip(valid_bleu, max_bleu)):\n",
        "                max_bleu = valid_bleu\n",
        "            else:\n",
        "                print(\"None of the BLEU scores increased, patience down: \", patience)\n",
        "                patience -= 1\n",
        "\n",
        "            print(valid_bleu)\n",
        "\n",
        "            encoder_decoder.set_decoder_train_mode(True)\n",
        "\n",
        "    if patience == 0:\n",
        "        print(\"Training stopped due to early stopping.\")\n",
        "        break\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TestDataset(Dataset):\n",
        "    def __init__(self, split_file):\n",
        "        self.image_dir = \"Flickr8k_images\"\n",
        "        self.data = self.load_data(split_file)\n",
        "        self.setup_transforms()\n",
        "\n",
        "    def setup_transforms(self):\n",
        "        self.show_transform = transforms.Compose([\n",
        "            transforms.Resize(IMG_SIZE),\n",
        "            transforms.CenterCrop(IMG_SIZE),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "        self.normalize = transforms.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225]\n",
        "        )\n",
        "        self.model_transform = transforms.Compose([\n",
        "            transforms.Resize(IMG_SIZE),\n",
        "            transforms.CenterCrop(IMG_SIZE),\n",
        "            transforms.ToTensor(),\n",
        "            self.normalize,\n",
        "        ])\n",
        "\n",
        "    def load_data(self, split_file):\n",
        "        with open(split_file, \"r\") as f:\n",
        "            data = [line.replace(\"\\n\", \"\") for line in f.readlines()]\n",
        "        return self.group_captions(data)\n",
        "\n",
        "    def group_captions(self, data):\n",
        "        grouped_captions = {}\n",
        "        for line in data:\n",
        "            caption_data = line.split()\n",
        "            img_name, img_caption = caption_data[0].split(\"#\")[0], caption_data[1:]\n",
        "            if img_name not in grouped_captions:\n",
        "                grouped_captions[img_name] = []\n",
        "            grouped_captions[img_name].append(img_caption)\n",
        "        return grouped_captions\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        img_id = list(self.data.keys())[idx]\n",
        "        caption_words_list = self.data[img_id]\n",
        "        img_path = os.path.join(self.image_dir, img_id)\n",
        "        raw_image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        image_tensor = self.model_transform(raw_image)\n",
        "        raw_image_tensor = self.show_transform(raw_image)\n",
        "\n",
        "        return image_tensor, raw_image_tensor, caption_words_list\n",
        "\n",
        "    def display_image(self, image_tensor):\n",
        "        image_array = image_tensor.permute(1, 2, 0).numpy()\n",
        "        image_array = (image_array * 255).astype('uint8')\n",
        "        plt.imshow(image_array)\n",
        "        plt.axis('off')\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "9fxueoDn3R69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdy3SNKxf0uI",
        "outputId": "3227c887-abce-4639-aad7-57d06e3bcc22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating model.\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "woman and white dog and white and white dog is standing on her dog\n",
            "\n",
            "Actual Caption:\n",
            "girl in pink shirt riding large white dog indoors\n",
            "child on big dog looking towards camera inside house\n",
            "young girl sitting on back of white furry dog\n",
            "large white dog carrying girl in pink indoors\n",
            "indoors girl wearing hat rides on big white dog\n",
            "------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------\n",
            "Predicted Caption:\n",
            "woman with black and black dog with sunglasses is wearing blue jacket and black and black and red shirt and red shirt and white shirt is wearing white shirt and black shirt and white shirt and white shirt is standing on the sand\n",
            "\n",
            "Actual Caption:\n",
            "tan dog wearing red sunglasses on beach\n",
            "small dog with tongue out sitting on sand\n",
            "happy dog in sunglasses laying on beach\n",
            "dog wearing shades relaxing by ocean\n",
            "sunny day dog with red glasses on shore\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "dog is running on the grass\n",
            "\n",
            "Actual Caption:\n",
            "dog chasing ball on grassy field\n",
            "two dogs playing catch outside\n",
            "brown dog running after pink ball\n",
            "large white dog walking on grass\n",
            "people watching dogs play in park\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "two dogs are playing in the grass\n",
            "\n",
            "Actual Caption:\n",
            "three dogs playing together in park area\n",
            "woman standing near dogs on sandy ground\n",
            "one black dog carrying stick others watching\n",
            "people and dogs enjoy sunny day outside\n",
            "dogs interact near trees and fence in park\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "two dogs are playing in the snow\n",
            "\n",
            "Actual Caption:\n",
            "four dogs sitting in snowy area\n",
            "black and white dogs facing camera\n",
            "two brown dogs two black dogs\n",
            "dogs wearing collars stand in snow\n",
            "one dog looks away in snowy field\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "brown dog is walking on the grass\n",
            "\n",
            "Actual Caption:\n",
            "golden dog lying on red bed indoors\n",
            "happy dog sitting on large cushion\n",
            "dog resting indoors in sunny room\n",
            "orange dog relaxing on textured bed\n",
            "large dog sitting looking at camera\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "dog is running on the beach\n",
            "\n",
            "Actual Caption:\n",
            "dog standing on sandy beach near water\n",
            "black and brown dog looking towards camera on beach\n",
            "tricolored dog with wet fur standing by ocean\n",
            "happy dog on beach at sunset near ocean waves\n",
            "one dog enjoying walk along shore of sandy beach\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "dog running in the grass\n",
            "\n",
            "Actual Caption:\n",
            "brown white dog running across snowy field\n",
            "happy dog playing alone in snow\n",
            "one dog leaping through snow looking forward\n",
            "joyful small dog running on snowy ground\n",
            "brown and white dog enjoying snowy day outdoors\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "dog is running in the grass\n",
            "\n",
            "Actual Caption:\n",
            "golden dog lying on grass with tennis ball\n",
            "dog on grassy field with ball near front paw\n",
            "happy dog with ball resting on green lawn\n",
            "large dog playing with yellow ball on grass\n",
            "dog laying outdoors tennis ball next to paw\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "dog is running in the air\n",
            "\n",
            "Actual Caption:\n",
            "brown dog jumping near water grassy background\n",
            "dog leaping in air water puddle sunny day\n",
            "wet brown dog midair jump outdoors park\n",
            "happy dog outdoor jump near water sunny\n",
            "flying dog over water grass sunny park\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "dog is playing on the grass\n",
            "\n",
            "Actual Caption:\n",
            "two dogs indoors playing with toys\n",
            "brown dog chewing blue toy\n",
            "small white puppy holding orange toy\n",
            "dogs sit on rug beside chair\n",
            "puppy and dog playing together indoors\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "dog is running through the snow\n",
            "\n",
            "Actual Caption:\n",
            "golden dog running joyfully in snowy field\n",
            "happy dog leaping through deep snow outdoors\n",
            "smiling dog wearing collar enjoying snowy day\n",
            "snowcovered golden dog running in white landscape\n",
            "winter playtime with dog bounding across snow\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "white white dog is running through the grass\n",
            "\n",
            "Actual Caption:\n",
            "white dog running across green grassy field\n",
            "small dog leaping through air playing outside\n",
            "happy dog with floppy ears running in park\n",
            "energetic puppy enjoying playtime on grass outdoors\n",
            "dog in midjump over grass ears flying\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "white dog is running in the air\n",
            "\n",
            "Actual Caption:\n",
            "white dog catching tennis ball indoors\n",
            "dog leaping high tennis ball in mouth\n",
            "playing catch inside dog grabs ball\n",
            "indoor playtime dog jumping for ball\n",
            "white dog midair mouth catching ball\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "two dogs are running in the beach\n",
            "\n",
            "Actual Caption:\n",
            "dog walking near ocean water edge\n",
            "brown dog on sandy beach people swimming\n",
            "dog on beach looking towards ocean\n",
            "wet brown dog standing by water on beach\n",
            "people in water brown dog on sand\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "brown dog is running on the grass\n",
            "\n",
            "Actual Caption:\n",
            "man lying on grass playing with tan dog\n",
            "happy dog and man enjoying play outdoors\n",
            "man and tan dog having fun on lawn\n",
            "playful dog sitting on mans chest outside\n",
            "smiling man and dog in playful wrestling on grass\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "two dogs in snow\n",
            "\n",
            "Actual Caption:\n",
            "man in yellow throwing snow at dog\n",
            "dog playing catch with snow outdoors\n",
            "person and dog enjoy snowy day together\n",
            "happy dog catching flying snow from man\n",
            "man in jacket throws snowball at dog\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "brown dog is running on the grass\n",
            "\n",
            "Actual Caption:\n",
            "brown dog running indoors holding purple ball\n",
            "dog carrying ball across room looking forward\n",
            "happy dog playing fetch inside house\n",
            "brown dog with toy running toward camera\n",
            "playful dog inside running with ball in mouth\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "dog is running in the grass\n",
            "\n",
            "Actual Caption:\n",
            "small dog lying inside house\n",
            "puppy with red collar indoors\n",
            "young dog resting on carpet\n",
            "golden puppy holding red toy\n",
            "fluffy dog looking at camera\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "brown dog is running through the snow\n",
            "\n",
            "Actual Caption:\n",
            "dog jumping in snowy field\n",
            "brown and white dog playing in snow\n",
            "happy dog leaping towards camera in snow\n",
            "small dog catching snow midleap\n",
            "energetic dog with tongue out jumping in snow\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "black dog is running in the snow\n",
            "\n",
            "Actual Caption:\n",
            "two dogs playing in snowy field\n",
            "black and white dogs running through snow\n",
            "dogs enjoy running on snowcovered path\n",
            "white dog chasing black dog across snow\n",
            "joyful dogs play together in winter snow\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "two men playing in the grass\n",
            "\n",
            "Actual Caption:\n",
            "boy playing ball with jumping dog on grass\n",
            "dog jumping toward ball boy is holding\n",
            "child and dog play together in yard\n",
            "happy boy and dog jumping and playing\n",
            "boy and white dog running playing ball\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "two dogs are playing in the field\n",
            "\n",
            "Actual Caption:\n",
            "three dogs running together one carrying tennis ball\n",
            "brown and white dogs playing with yellow ball\n",
            "one dog leading playtime chase holding ball in mouth\n",
            "dogs enjoying game of fetch in sunny grassy field\n",
            "trio of beagle dogs engaged in playful run outdoors\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "dog is running through water\n",
            "\n",
            "Actual Caption:\n",
            "dog shaking off water ball near foot\n",
            "golden dog playing splashing water around\n",
            "wet dog standing on grass ball nearby\n",
            "dog shaking water ball on ground\n",
            "happy dog getting dry playing outside\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "woman in red dress and white and white and white and white and woman are are are are are are in front in the air\n",
            "\n",
            "Actual Caption:\n",
            "people dancing in colorful feathered costumes\n",
            "group performing dance in bright carnival attire\n",
            "women wearing colorful outfits dancing at event\n",
            "performers in elaborate costumes dancing together\n",
            "dancers with feathered headpieces smiling brightly\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "woman in red and pink shirt is wearing red and white shirt and white and white and woman is wearing white shirt and white and white shirt and white and white shirt is walking in the street\n",
            "\n",
            "Actual Caption:\n",
            "woman dancing in colorful costume at carnival\n",
            "people in background watching woman dance\n",
            "woman wearing vibrant orange and green outfit\n",
            "crowd looking at woman performing at event\n",
            "dressed in feathered costume woman celebrates outdoors\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "man in red and white and white and white shirt is wearing white and white and white and white and white and white and white hat is are are are standing in white and white and white and white and white and white and white and black and white shirt and white and white and white and white and\n",
            "\n",
            "Actual Caption:\n",
            "group wearing orange dancing costume smiling\n",
            "people in colorful outfits performing dance\n",
            "crowd watching group dressed in orange black\n",
            "several people wearing headscarf beads laughing\n",
            "dancers in striped costume laughing together\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "two children are in the air\n",
            "\n",
            "Actual Caption:\n",
            "people wearing colorful costumes performing\n",
            "crowd watching group in shiny outfits\n",
            "person in blue outfit posing\n",
            "performing at night dressed in elaborate costume\n",
            "group dressed in gold and purple dancing\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "group of people are are in front of people are in front of street\n",
            "\n",
            "Actual Caption:\n",
            "group dressed in colorful costumes dancing\n",
            "people wearing bright outfits performing\n",
            "crowd watching parade of dancers\n",
            "many smiling faces in costume\n",
            "performers with feathered headdresses at event\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "woman in red suit and white hat and black and yellow shirt is wearing white shirt is in the camera\n",
            "\n",
            "Actual Caption:\n",
            "woman in feathered costume dancing at carnival\n",
            "person wearing colorful outfit smiling at event\n",
            "woman in bikini and elaborate headdress performing\n",
            "dancer wearing blue and green feathers at parade\n",
            "smiling woman with large feathered wings at festival\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "woman in pink dress and white shirt is wearing white dress and white shirt in white shirt is in the camera\n",
            "\n",
            "Actual Caption:\n",
            "woman wearing green dress dancing in parade\n",
            "crowd wearing colorful costumes performing at event\n",
            "dancer in green dress smiling arms open wide\n",
            "group dressed in purple and gold marching behind woman\n",
            "performing woman in costume leading parade celebration\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "two people in red and white and woman are wearing white and white and white and black hair and black and white and woman is are standing\n",
            "\n",
            "Actual Caption:\n",
            "two women wearing costume smiling at night\n",
            "women wearing red and blue feathers smiling\n",
            "two people in carnival costumes night street\n",
            "people in elaborate costumes dancing outdoors\n",
            "women with large feathered headpieces at event\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "woman in pink dress and pink and white dress and white dress and white dress\n",
            "\n",
            "Actual Caption:\n",
            "people wearing costume dancing inside room\n",
            "man wearing hat holding woman wearing mask\n",
            "group dressed in colorful costumes indoor event\n",
            "men and women performing dance wearing historical clothing\n",
            "woman with blue dress dancing with man\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "woman in red shirt and red and blue and white and red hat and white and red dress is holding her hair and white dress\n",
            "\n",
            "Actual Caption:\n",
            "people wearing colorful costumes dancing on city street\n",
            "crowd watching two people performing at outdoor event\n",
            "man and woman in costume holding masks at parade\n",
            "group gathers to watch festival dancers in vibrant attire\n",
            "performers in masks dance in front of audience on street\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "two girls are are in white and white dress are standing\n",
            "\n",
            "Actual Caption:\n",
            "people wearing costume dancing inside building\n",
            "group dressed in traditional attire indoors\n",
            "men and women in elaborate outfits dance floor\n",
            "many people in period clothing enjoying event\n",
            "crowd celebrating in historical dress inside room\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "two people are in front of street\n",
            "\n",
            "Actual Caption:\n",
            "people wearing colorful costumes walking in parade\n",
            "crowd watching performers at street event\n",
            "large figure on float behind colorful parade\n",
            "performers in bright outfits dancing on road\n",
            "many spectators gathered at festive street celebration\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "group of people are are playing in the street\n",
            "\n",
            "Actual Caption:\n",
            "people wearing colorful costumes dancing indoors\n",
            "group performing dance wearing masks at event\n",
            "several people in bright outfits dancing inside\n",
            "costumed group performing at indoor celebration\n",
            "dancers wearing masks and colorful attire on floor\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "people in red hat and red and red and red and red hat and red hat is standing in front of red street\n",
            "\n",
            "Actual Caption:\n",
            "people in colorful costumes posing on city square\n",
            "group wearing masks and elaborate outfits at event\n",
            "several dressed up standing together outdoor celebration\n",
            "crowd in bright dresses and costumes building background\n",
            "masked figures in ornate attire gathering in public space\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "people in red and red and people are are walking in the street\n",
            "\n",
            "Actual Caption:\n",
            "people wearing colorful costumes performing in street\n",
            "people in elaborate outfits dancing outside\n",
            "crowd watching performers in traditional attire\n",
            "group dressed in bright historical clothing dancing\n",
            "performers in vibrant costumes entertain onlookers outdoors\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "two people in people are are standing in the street\n",
            "\n",
            "Actual Caption:\n",
            "people wearing colorful costumes and masks outdoors\n",
            "group holding sticks dressed in elaborate outfits\n",
            "crowd in vibrant feathered masks near building\n",
            "several people in masks posing at event\n",
            "men and women in costume celebrate on street\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "group of people are in the street\n",
            "\n",
            "Actual Caption:\n",
            "people dressed in costume at event\n",
            "group wearing colorful historical attire inside\n",
            "people in elaborate costumes celebrating together\n",
            "crowd gathered in costumes sitting standing indoors\n",
            "many in fancy dress at decorated table\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "woman in pink suit is walking in the air\n",
            "\n",
            "Actual Caption:\n",
            "woman wearing red costume dancing at parade\n",
            "group wearing colorful costumes performing outdoors\n",
            "dancer in red outfit smiling in street event\n",
            "woman with feathered headpiece leading parade\n",
            "performers in bright attire celebrating on road\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "group of people are are in the red and white and white and white and white and white and white and white and white and white and white and white and white and white and white and white and white and white and white and white and white and white and white and white and white dress are are\n",
            "\n",
            "Actual Caption:\n",
            "group wearing yellow purple costumes dancing street parade\n",
            "people in colorful outfits performing dance in city\n",
            "crowd watching group dressed in purple yellow dancing\n",
            "several people wearing bright costumes dance during event\n",
            "group dressed in purple yellow performing street dance\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "group of people are in the women are walking in front of people are are are are in the street\n",
            "\n",
            "Actual Caption:\n",
            "group of women dancing in colorful costumes\n",
            "women wearing feathered headdresses parading street\n",
            "several smiling ladies performing dance in daylight\n",
            "costumeclad group celebrating outdoors with dance\n",
            "crowd of dancers wearing elaborate vibrant attire\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "two girls in green and green and green and girls are walking in the street\n",
            "\n",
            "Actual Caption:\n",
            "girl in green costume dancing on street\n",
            "children performing dance at outdoor event\n",
            "group wearing colorful costumes at parade\n",
            "young dancers celebrating in city street\n",
            "crowd watching kids dance performance outdoors\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "group of people are in green street\n",
            "\n",
            "Actual Caption:\n",
            "women wearing colorful costumes dancing in parade\n",
            "people in bright outfits performing at carnival\n",
            "group dancing on street with elaborate headpieces\n",
            "crowd watching women dance in golden attire\n",
            "performers wearing orange and gold at outdoor event\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "two children in pink and pink dress and pink and pink and pink dress and blue dress is are are standing in the street\n",
            "\n",
            "Actual Caption:\n",
            "group of women dancing on street wearing colorful costumes\n",
            "women in bright outfits performing dance outdoors\n",
            "several smiling women parading in festive attire\n",
            "crowd of ladies walking in costumes with feathers\n",
            "people celebrate outside in vibrant decorated clothing\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "two women in red and red and white and white and white dress\n",
            "\n",
            "Actual Caption:\n",
            "women wearing red yellow costumes dancing in parade\n",
            "group of people in colorful outfits performing onstage\n",
            "several dancers with feathered headpieces at carnival event\n",
            "women posing in bright dance costumes during festival\n",
            "crowd in elaborate attire celebrating with dance and music\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "two people in red and red and white and white and red shirt and red hat\n",
            "\n",
            "Actual Caption:\n",
            "group of people dancing in colorful costumes on street\n",
            "women wearing bright yellow dresses performing dance\n",
            "crowd watching dancers in vibrant outfits during daylight\n",
            "several dancers with golden headbands dance outdoors\n",
            "dancers in festive attire smiling and holding hands\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "people are sitting on the street\n",
            "\n",
            "Actual Caption:\n",
            "crowd walking down subway station stairs\n",
            "people descending steps at city subway entrance\n",
            "group entering underground station on busy street\n",
            "many individuals on staircase leading to subway\n",
            "crowd gathered at subway station entrance outdoors\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "group of people in the crowd\n",
            "\n",
            "Actual Caption:\n",
            "crowd of people holding objects looking up\n",
            "group of men and women watching together\n",
            "many people raising hands holding items\n",
            "people in crowd looking upwards holding mirrors\n",
            "group of people standing looking holding glasses\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "group of people in the street\n",
            "\n",
            "Actual Caption:\n",
            "man wearing black jacket walking across city street\n",
            "young person with sunglasses crossing road people around\n",
            "person in black crossing busy intersection looking forward\n",
            "man in dark outfit walks through crowd on crosswalk\n",
            "stylish guy with black leather jacket in urban setting\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "two people in the street\n",
            "\n",
            "Actual Caption:\n",
            "people walking on city street in daylight\n",
            "group of women and man on sidewalk\n",
            "crowd walking together on sunny day\n",
            "several people near crosswalk in city\n",
            "pedestrians wearing jackets and sunglasses\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "people in the people are standing in the street\n",
            "\n",
            "Actual Caption:\n",
            "woman in black coat standing among crowd\n",
            "people walking on busy city street at night\n",
            "woman looking at camera with city lights behind\n",
            "crowded street with woman in black standing still\n",
            "young woman facing camera on populated sidewalk\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "man in the shirt and man is standing on the street\n",
            "\n",
            "Actual Caption:\n",
            "man standing alone amidst walking crowd\n",
            "man wearing green shirt looking camera\n",
            "one man still while others blur around\n",
            "person holding bag in crowded street\n",
            "young man surrounded by moving people\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "man in white shirt is standing in front of street\n",
            "\n",
            "Actual Caption:\n",
            "man holding child standing in crowded street\n",
            "people walking along busy city market at night\n",
            "crowd gathered in street with colorful lights and signs\n",
            "night market scene with man and child looking away\n",
            "several people in busy street market near shops\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "group of people are standing in the street\n",
            "\n",
            "Actual Caption:\n",
            "many people walking on busy city street\n",
            "crowd of people walking along sidewalk\n",
            "group walking near buildings and shops\n",
            "busy city street with walking crowd\n",
            "people on street near shops in city\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "people are standing on the street\n",
            "\n",
            "Actual Caption:\n",
            "crowded city street with many people walking\n",
            "many people walking along busy urban sidewalk\n",
            "busy city scene with crowd of people outdoors\n",
            "group of people walking past shops on city street\n",
            "several people in crowd walking on city road\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "man and man are standing in the dog\n",
            "\n",
            "Actual Caption:\n",
            "people walking across the street in city\n",
            "crowd crossing road on painted crosswalk\n",
            "many people wearing coats in urban setting\n",
            "group of pedestrians walking on street together\n",
            "busy city street with walking crowd outdoors\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "group of people are sitting in the air\n",
            "\n",
            "Actual Caption:\n",
            "people sitting outside eating at street tables\n",
            "man standing cooking on grill smoke rising\n",
            "group enjoying food man grilling in background\n",
            "people dining on sidewalk street smoke surrounds\n",
            "men and women at outdoor restaurant city life\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "woman in black shirt is standing in front of the street\n",
            "\n",
            "Actual Caption:\n",
            "woman standing inside bus holding pole looking away\n",
            "people sitting reading holding inside crowded bus\n",
            "young woman in blue dress standing bus passengers behind\n",
            "passengers on bus sitting standing woman in denim shirt\n",
            "inside bus girl holds pole riders sit read talk\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "man in red shirt is holding red shirt\n",
            "\n",
            "Actual Caption:\n",
            "young man holding phone standing in subway\n",
            "people riding subway looking at phones\n",
            "crowd inside train during busy hour\n",
            "person with camera riding crowded subway\n",
            "subway passengers standing holding onto handles\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "group of people in front of people are standing in front of people in front of street\n",
            "\n",
            "Actual Caption:\n",
            "people walking on sunny city street\n",
            "woman looking at camera on busy sidewalk\n",
            "group of people walking along shop fronts\n",
            "man and woman standing in sunny street\n",
            "several people walking shopping in daylight\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "group of people are standing in front of street\n",
            "\n",
            "Actual Caption:\n",
            "woman looking back crowd walking on street\n",
            "blonde woman standing among walking people\n",
            "woman in coat looking cherry blossoms behind\n",
            "person standing out in busy city crowd\n",
            "young woman turns standing in crowded area\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "man in the street\n",
            "\n",
            "Actual Caption:\n",
            "people walking across city street crosswalk\n",
            "group of pedestrians crossing urban road\n",
            "men and women walk on striped pavement\n",
            "crowd navigating through city crosswalk\n",
            "busy city street with walking people\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "two people are sitting in front of window\n",
            "\n",
            "Actual Caption:\n",
            "people sitting inside train looking at cellphones\n",
            "group holding smartphones inside subway car\n",
            "men and woman using phones on public transport\n",
            "several passengers focused on devices on train\n",
            "young adults engaged with mobile phones on commute\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "people in white shirt is standing on the street\n",
            "\n",
            "Actual Caption:\n",
            "people standing inside crowded subway train\n",
            "men and women looking at cellphones on train\n",
            "group holding onto handrails in train car\n",
            "commuters ride together busy with phones\n",
            "subway interior filled with passengers during ride\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "man in blue shirt is sitting on the street\n",
            "\n",
            "Actual Caption:\n",
            "people sitting reading newspapers inside subway train\n",
            "group of commuters holding onto yellow pole in subway\n",
            "men and women looking at papers while traveling underground\n",
            "subway passengers seated and standing during commute\n",
            "crowd inside train car with several reading metro newspaper\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "two people are sitting in the camera\n",
            "\n",
            "Actual Caption:\n",
            "group of people sitting together looking different directions\n",
            "several young men and women holding books phones\n",
            "crowd of young adults dressed in colorful clothing\n",
            "people with glasses reading using cellphone looking away\n",
            "group seated closely showing various facial expressions activities\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "group of people are are are in the camera\n",
            "\n",
            "Actual Caption:\n",
            "people standing together wearing colorful clothes\n",
            "group of people looking different directions\n",
            "crowd wearing various outfits different colors\n",
            "many individuals gathered holding different objects\n",
            "diverse group standing talking dressed in colors\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "group of people are in the street\n",
            "\n",
            "Actual Caption:\n",
            "people walking on busy city street\n",
            "crowd crossing road in urban area\n",
            "group of people near crosswalk city\n",
            "men and women walking along sidewalk\n",
            "several people walking across street\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "man in red shirt is walking on street\n",
            "\n",
            "Actual Caption:\n",
            "people walking across city street crosswalk\n",
            "men crossing road yellow car behind\n",
            "city background group of people walking\n",
            "crowded city street pedestrians crossing road\n",
            "busy intersection pedestrians walk taxis wait\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "group of people are are in the people are are are are are are sitting in the background\n",
            "\n",
            "Actual Caption:\n",
            "woman in red dress standing among crowd\n",
            "crowd of people one woman looking up\n",
            "group of people lady in red standing out\n",
            "many people together woman in red visible\n",
            "crowded scene young woman looking upwards\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "group of people are are are are in crowd\n",
            "\n",
            "Actual Caption:\n",
            "crowd of people standing together outside during day\n",
            "many young people wearing colorful clothes gathered\n",
            "large group outside wearing hats looking in distance\n",
            "several men and women in casual clothing daylight\n",
            "outdoor event with group of people various outfits\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "man is riding motorcycle\n",
            "\n",
            "Actual Caption:\n",
            "man riding motorcycle racing on road\n",
            "motorcyclist wearing red leaning in turn\n",
            "person racing bike on track wearing helmet\n",
            "rider on motorcycle wearing colorful gear\n",
            "motorcycle race rider wearing red suit\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "man in black motorcycle is riding on motorcycle\n",
            "\n",
            "Actual Caption:\n",
            "man and woman smiling riding motorcycle together\n",
            "two people on motorcycle wearing dark jackets\n",
            "couple enjoying motorcycle ride on sunny day\n",
            "motorcycle with rider and passenger on street\n",
            "riding motorcycle man in front woman behind\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "man is riding motorcycle on dirt road\n",
            "\n",
            "Actual Caption:\n",
            "man riding motorcycle on curvy mountain road\n",
            "motorcyclist leaning into turn on paved street\n",
            "rider wearing helmet driving fast along road\n",
            "person on motorcycle cornering on country road\n",
            "motorcycle and rider speeding on twisting road\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "motorcycle on motorcycle is riding\n",
            "\n",
            "Actual Caption:\n",
            "man riding motorcycle on racetrack\n",
            "rider wearing helmet racing bike\n",
            "motorcycle leaning in turn on track\n",
            "racer in black suit driving fast\n",
            "speeding motorcyclist wearing gear on road\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "man is riding motorcycle on the motorcycle\n",
            "\n",
            "Actual Caption:\n",
            "person riding yellow motorcycle on sunny city street\n",
            "man wearing helmet driving yellow motorcycle near water\n",
            "rider on yellow motorcycle passing car on bridge\n",
            "motorcycle driving on road along blue water and city\n",
            "yellow motorcycle overtaking black car on urban road\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "two people are on the grass\n",
            "\n",
            "Actual Caption:\n",
            "man wearing helmet riding motorcycle near grass\n",
            "motorcyclist in black leather jacket on bike\n",
            "rider on classic motorcycle driving along grassy area\n",
            "person on motorcycle wearing sunglasses riding past green field\n",
            "man driving silver and black motorcycle in sunny weather\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "two people are riding dirt\n",
            "\n",
            "Actual Caption:\n",
            "man riding motorcycle racing dirt track\n",
            "racer wearing helmet jumps motorcycle midair\n",
            "motorcycle performing jump rider wearing yellow\n",
            "person on blue motorcycle racing dirty course\n",
            "motorcyclist in air racing fast on track\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "man is riding the bike\n",
            "\n",
            "Actual Caption:\n",
            "man riding motorcycle near ocean watching airplane\n",
            "motorcycle standing on grass overlooking rocky shore\n",
            "rider in blue looking at plane above water\n",
            "people on coast watch motorcycle rider by sea\n",
            "motorcyclist observes plane flying past ocean pier\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "man in black and black shirt and black shirt is riding on the bike\n",
            "\n",
            "Actual Caption:\n",
            "man sitting on silver motorcycle wearing blue jeans\n",
            "man with motorcycle near green bush smiling\n",
            "person on american flagdecorated chopper in street\n",
            "motorcycle rider posing beside chrome vehicle blue sky\n",
            "man holding motorcycle handlebars long front wheel\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "man wearing woman is riding on red and black and black and red helmet\n",
            "\n",
            "Actual Caption:\n",
            "man wearing black standing next to motorcycle\n",
            "person looking back while holding motorcycle\n",
            "man in leather jacket with red motorcycle on road\n",
            "motorcycle rider standing looking over shoulder\n",
            "rider pausing on road beside red and silver motorcycle\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "two people are riding on the motorcycle\n",
            "\n",
            "Actual Caption:\n",
            "woman sitting on motorcycle in city at sunset\n",
            "young lady rides black motorcycle near buildings\n",
            "motorcycle rider pausing ride city in background\n",
            "woman atop parked motorcycle urban skyline behind\n",
            "female biker taking break tall city structures\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "two people are riding motorcycle on the dirt\n",
            "\n",
            "Actual Caption:\n",
            "man sitting on motorcycle smiling\n",
            "person wearing black leather jacket on bike\n",
            "motorcyclist beside road near grassy hill\n",
            "smiling man with motorcycle resting\n",
            "rider with black motorcycle standing on road\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "man and man and black shirt and black and black and black and black shirt and black shirt and black shirt and black and black shirt and black and black and black and black shirt and black and black and black shirt and black and black shirt and black and black and black and black and black and black\n",
            "\n",
            "Actual Caption:\n",
            "man sitting on black motorcycle near gray car\n",
            "man wearing vest and sunglasses poses on bike\n",
            "person with beard resting on motorcycle in city\n",
            "sunglasswearing male sits on parked motorcycle\n",
            "bearded man in colorful pants on black motorcycle\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "two people are are on motorcycle\n",
            "\n",
            "Actual Caption:\n",
            "police officer riding motorcycle on road\n",
            "person wearing helmet driving police bike\n",
            "man on police motorcycle patrolling street\n",
            "policeman on bike with flashing lights\n",
            "police rider in uniform on motorbike\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "man and two motorcycle on motorcycle\n",
            "\n",
            "Actual Caption:\n",
            "man sitting on road near motorcycle looking at phone\n",
            "person with helmet sits beside parked motorcycle\n",
            "motorcycle driver on pavement using cellphone\n",
            "motorcyclist resting on ground next to bike\n",
            "young man in sitting position by motorcycle on street\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "man is riding motorcycle\n",
            "\n",
            "Actual Caption:\n",
            "man wearing helmet riding red motorcycle racing\n",
            "motorcycle racer leaning into turn on track\n",
            "racing motorcycle ridden by man in full gear\n",
            "man racing on motorcycle wearing black suit\n",
            "motorcycle with number racing along racetrack\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "man in red and red and white and black and black and black shirt and black and white and white helmet is riding on the black and white helmet and motorcycle is riding motorcycle\n",
            "\n",
            "Actual Caption:\n",
            "man wearing helmet sitting on racing motorcycle\n",
            "smiling man in racing suit on motorcycle\n",
            "racer sitting on bike holding helmet on track\n",
            "man with race number sitting on red motorcycle\n",
            "motorcycle rider holding helmet sitting on racetrack\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "man and black shirt and red and black and black and white shirt and black and black and black and white and white shirt and white helmet and white and black shirt and white shirt is riding black and black and red and white shirt and black and black shirt is riding on black and white and black and\n",
            "\n",
            "Actual Caption:\n",
            "person riding red motorcycle on curvy road\n",
            "man wearing helmet driving motorcycle along street\n",
            "motorcyclist leaning into turn on rural road\n",
            "red motorcycle with rider on paved winding path\n",
            "helmeted person on red bike takes sharp corner\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "two people are riding on the motorcycle\n",
            "\n",
            "Actual Caption:\n",
            "man riding red motorcycle on road wearing helmet\n",
            "person driving red bike along street in sunlight\n",
            "rider on red sport motorcycle wearing white helmet\n",
            "motorcyclist in jeans and jacket races on highway\n",
            "man with helmet rides red racing motorcycle outside\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "two people are riding motorcycle\n",
            "\n",
            "Actual Caption:\n",
            "man wearing green helmet riding green motorcycle\n",
            "person riding motorcycle wearing matching helmet and suit\n",
            "rider on green and black motorcycle wearing helmet\n",
            "motorcyclist in green gear riding on street\n",
            "man on motorcycle wearing green black outfit\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "two men are are riding on motorcycle\n",
            "\n",
            "Actual Caption:\n",
            "man riding motorcycle on city road\n",
            "person wearing helmet driving bike fast\n",
            "motorcycle moving along street under bridge\n",
            "rider on black motorcycle wearing jacket\n",
            "male on motorcycle speeding past city background\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "man in red shirt and white shirt and white helmet is riding on the street\n",
            "\n",
            "Actual Caption:\n",
            "man riding motorcycle racing near crowd watching\n",
            "person in white racing old motorcycle on street\n",
            "motorcycle racer wearing helmet competing in race\n",
            "driver on vintage bike wearing helmet near people\n",
            "racing event with motorcycle rider in white outfit\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "man is riding bike on the dirt\n",
            "\n",
            "Actual Caption:\n",
            "man wearing helmet riding motorcycle on dirt road\n",
            "rider in black gear on electric motorcycle outdoors\n",
            "motorcyclist leaning on bike in rocky terrain\n",
            "person on twowheeled vehicle preparing for race\n",
            "man dressed in protective clothing riding motorbike offroad\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "two people are are on the water\n",
            "\n",
            "Actual Caption:\n",
            "three children riding one motorcycle on road\n",
            "boy driving motorcycle with two young passengers\n",
            "young boy carrying two others on motorcycle\n",
            "small children sharing ride on single motorcycle\n",
            "kids riding together on motorcycle down street\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "person is riding bike on dirt\n",
            "\n",
            "Actual Caption:\n",
            "man riding motorcycle jumping dirt ramp\n",
            "motorcycle flying midair above dirt track\n",
            "rider performing jump on motocross bike\n",
            "motocross racer airborne over raceway\n",
            "man wearing red racing midair on motorcycle\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "the basketball player in the the red shirt is is in the blue shirt\n",
            "\n",
            "Actual Caption:\n",
            "two girls playing basketball indoors\n",
            "basketball game indoors girl holding ball\n",
            "girls in blue uniforms playing basketball\n",
            "young basketball players competing indoors\n",
            "female basketball players in action indoors\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "the basketball is playing basketball\n",
            "\n",
            "Actual Caption:\n",
            "two men playing basketball on indoor court\n",
            "players compete for ball during basketball game\n",
            "basketball game intense moment between two players\n",
            "man holding basketball on floor another player near\n",
            "basketball players in uniform one lying ground\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "man in white and white shirt is standing on the air\n",
            "\n",
            "Actual Caption:\n",
            "crowd watching basketball game indoors\n",
            "basketball players competing on court\n",
            "indoor basketball court with spectators\n",
            "group of people cheering basketball team\n",
            "full basketball stadium during game\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "man in the white uniform is playing to basketball\n",
            "\n",
            "Actual Caption:\n",
            "two men playing basketball one shooting towards hoop\n",
            "basketball player in white jumping to score against opponent\n",
            "man in red defends while another in white shoots basketball\n",
            "players compete in basketball game jumping near basket\n",
            "basketball game action player in white attempts to score\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "boy in red shirt is playing to basketball\n",
            "\n",
            "Actual Caption:\n",
            "man wearing white playing basketball dunk\n",
            "basketball player in air throwing ball towards basket\n",
            "red team watching white uniform player dunk\n",
            "basketball game action shot above hoop\n",
            "players compete in basketball match indoor court\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "the player in red and white is playing basketball\n",
            "\n",
            "Actual Caption:\n",
            "men playing basketball on indoor court\n",
            "basketball player in white shooting ball\n",
            "two teams compete in basketball game\n",
            "man jumps high to score in basketball\n",
            "players watching teammate attempt basket\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "the the boy is in the air\n",
            "\n",
            "Actual Caption:\n",
            "aerial view of basketball court people playing\n",
            "basketball players on outdoor court game in action\n",
            "people scattered playing basketball overhead court view\n",
            "outdoor basketball court players engaging in sport\n",
            "basketball game underway birdseye view of court\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "the basketball player in the basketball\n",
            "\n",
            "Actual Caption:\n",
            "woman playing basketball indoors wearing white and green\n",
            "basketball player in white dribbling past defender in blue\n",
            "indoor basketball game woman in white uniform with ball\n",
            "basketball court action with two female players crowd watching\n",
            "female basketball player dribbles opponent defends spectators in background\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "basketball player in the basketball\n",
            "\n",
            "Actual Caption:\n",
            "player in yellow shooting basketball defender jumps to block\n",
            "basketball game indoors two men competing\n",
            "man in purple uniform attempting to score against opponent\n",
            "basketball player in yellow jumps shoots towards hoop\n",
            "athletes playing basketball crowd watching from sideline\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "the basketball player is in the basketball\n",
            "\n",
            "Actual Caption:\n",
            "young boy sitting on basketball court holding two balls\n",
            "child wearing blue shorts rests with basketballs\n",
            "little player looking away near basketball court\n",
            "boy with basketballs ready to play on green court\n",
            "kid holding sports equipment sitting on ground outside\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "group of children are in the street\n",
            "\n",
            "Actual Caption:\n",
            "men standing on basketball court holding ball\n",
            "basketball players in colorful sneakers ready to play\n",
            "group of young men on outdoor basketball court\n",
            "basketball team standing together wearing shorts\n",
            "young basketball players pose with ball outdoors\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "two men are playing in the ball\n",
            "\n",
            "Actual Caption:\n",
            "man in red uniform playing basketball\n",
            "two men watching another with ball\n",
            "basketball player dribbling past opponent\n",
            "athlete in red attempting to score\n",
            "basketball game in action on court\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "boy in the basketball and the uniform is playing in the air\n",
            "\n",
            "Actual Caption:\n",
            "two players in uniform playing basketball indoors\n",
            "girl dribbling basketball with teammate looking on\n",
            "basketball players wearing blue and orange uniforms\n",
            "young women playing basketball on indoor court\n",
            "team working together in basketball game\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "group of men in the people are are are are playing in the air\n",
            "\n",
            "Actual Caption:\n",
            "two teams playing basketball indoors\n",
            "players in uniform competing on court\n",
            "men jumping to catch basketball midair\n",
            "basketball game action inside gym\n",
            "group of men watching player jump ball\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "basketball player in white and red shirt is playing to basketball\n",
            "\n",
            "Actual Caption:\n",
            "basketball player shooting ball towards hoop\n",
            "crowd watching basketball game indoors\n",
            "players on court watch airborne basketball\n",
            "basketball game action with flying ball\n",
            "men in jerseys compete in basketball game\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "basketball player in the white uniform is playing to the ball\n",
            "\n",
            "Actual Caption:\n",
            "two players wearing white basketball uniforms playing\n",
            "person dribbling basketball wearing white uniform\n",
            "crowd watching two men play basketball indoor\n",
            "basketball player in white running holding ball\n",
            "men in white basketball uniforms competing indoors\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "basketball player in the basketball player is in the air\n",
            "\n",
            "Actual Caption:\n",
            "basketball team holding balls standing on court\n",
            "young players wearing black uniforms with basketballs\n",
            "basketball players pose together in gym holding balls\n",
            "team stands on basketball court holding balls ready to play\n",
            "group in black jerseys holding basketballs in indoor court\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "man is jumping to blue and white and blue and white dog\n",
            "\n",
            "Actual Caption:\n",
            "two people playing basketball on blue court\n",
            "kids playing basketball on outdoor court\n",
            "outdoor basketball court with playing kids\n",
            "basketball game on blue court sunny day\n",
            "children on blue basketball court with ball\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "man in white and white shirt and white and white shirt is playing in the air\n",
            "\n",
            "Actual Caption:\n",
            "man jumping toward hoop basketball game in action\n",
            "players competing in indoor basketball match\n",
            "athlete in white about to score in basketball game\n",
            "basketball player in midair shooting ball at hoop\n",
            "team in uniform playing basketball on indoor court\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "people in the ball in the air\n",
            "\n",
            "Actual Caption:\n",
            "indoor basketball game crowd watching scoreboard lit\n",
            "basketball court players ready fans filled stadium\n",
            "crowded sports arena basketball match team play\n",
            "people watching live basketball scoreboard shows score\n",
            "basketball competition in full swing excited audience watches\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "group of people are playing on the air\n",
            "\n",
            "Actual Caption:\n",
            "men playing basketball on outdoor court\n",
            "players in white shirts playing basketball\n",
            "basketball game on red and green court\n",
            "group of people watch basketball game\n",
            "outdoor basketball court with players and spectators\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "group of people in the street\n",
            "\n",
            "Actual Caption:\n",
            "people playing basketball on colorful court outdoors\n",
            "group of men wearing white shirts playing basketball\n",
            "outdoor basketball game on vibrant multicolored court\n",
            "men play basketball near city buildings under bridge\n",
            "basketball players on sunny day in urban area\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "the boy in white shirt is in red shirt is playing to white ball\n",
            "\n",
            "Actual Caption:\n",
            "two men playing basketball on green court outdoor\n",
            "man in green shirt dribbling basketball against another\n",
            "basketball game in action under sunny sky on court\n",
            "players wearing jerseys compete in basketball match outside\n",
            "basketball player with ball being defended on sunny court\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "man in red and red shirt is jumping to the air\n",
            "\n",
            "Actual Caption:\n",
            "two people playing basketball on outdoor court\n",
            "person in white shooting ball towards basketball hoop\n",
            "basketball game in action on sunny day\n",
            "overhead view of basketball player attempting to score\n",
            "outdoor basketball play on red and green court\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "two men are playing to the ball\n",
            "\n",
            "Actual Caption:\n",
            "young men playing basketball outdoors\n",
            "players on court one shooting basketball\n",
            "basketball game on outdoor court sunny day\n",
            "group of people watching street basketball game\n",
            "athletes in uniforms compete on basketball court\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "person is climbing rock\n",
            "\n",
            "Actual Caption:\n",
            "woman climbing snowy mountain rock in sunlight\n",
            "person in purple and red climbing steep cliff\n",
            "girl in helmet smiling whilst climbing rocky face\n",
            "climber with rope on snowy mountain during day\n",
            "female athlete climbing rock cliff snow in background\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "man in red shirt climbing\n",
            "\n",
            "Actual Caption:\n",
            "man climbing rocky mountain in sunny area\n",
            "man wearing red shorts climbing steep rock\n",
            "person on rock face looking towards peak\n",
            "climber in gray shirt scaling large cliff\n",
            "male athlete tackling climb on gray mountain\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "man is on the mountain\n",
            "\n",
            "Actual Caption:\n",
            "woman climbing rock mountain in sunny background\n",
            "person in pink shirt and helmet climbing steep cliff\n",
            "climber ascending rocky face against snowy mountain backdrop\n",
            "adventurous climber tackling challenging rocky slope outdoors\n",
            "female scaling large boulder with snowy peaks behind\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "man in red shirt is jumping on the air\n",
            "\n",
            "Actual Caption:\n",
            "man climbing rocky mountain wearing red shirt\n",
            "person wearing harness climbing steep rock\n",
            "climber in red holding onto mountain side\n",
            "young man ascending rough cliff outdoors\n",
            "athlete practicing rockclimbing on large wall\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "man in red shirt is climbing up rock\n",
            "\n",
            "Actual Caption:\n",
            "man climbing rocky mountain in sunny weather\n",
            "person wearing helmet climbing steep rock face\n",
            "man with rope climbing snowy mountain background\n",
            "climber in orange helmet smiling on mountain climb\n",
            "happy man climbing rock with snow below\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "man is climbing rock rock\n",
            "\n",
            "Actual Caption:\n",
            "man climbing large rocky mountain with rope\n",
            "person in blue shirt scaling steep rock face\n",
            "climber wearing helmet balancing between boulders\n",
            "man attempting challenging climb on cliff outdoors\n",
            "outdoor rock climbing man bridging two tall stones\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "man in red shirt is climbing up to rock\n",
            "\n",
            "Actual Caption:\n",
            "man climbing steep rocky mountain cliff\n",
            "person in blue shirt outdoors on rock\n",
            "climber with rope scaling high wall\n",
            "athletic individual gripping onto cliff face\n",
            "solo mountaineer ascending rugged terrain\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "man in red shirt and red shirt is climbing rock\n",
            "\n",
            "Actual Caption:\n",
            "woman climbing rocky mountain wearing helmet\n",
            "climber in orange pants moving up steep rock\n",
            "person gripping cliff side looking up towards top\n",
            "young female athlete climbing large rocky cliff\n",
            "adventure seeker scaling mountain with helmet on\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "man is riding rock on the air\n",
            "\n",
            "Actual Caption:\n",
            "person climbing rock ocean in background\n",
            "man on mountain city and water below\n",
            "climber in blue helmet on steep cliff\n",
            "adventurous climber large cityscape near ocean\n",
            "action shot of man climbing rocky mountain\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "man is climbing rock on rock\n",
            "\n",
            "Actual Caption:\n",
            "person climbing rock in green jacket and helmet\n",
            "man hanging from rock face with sky in background\n",
            "climber in midair against steep rocky mountain\n",
            "adventure climbing sports outdoors on rocky cliff\n",
            "rock climber with gear looking down while suspended\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "man in red jacket is climbing rock\n",
            "\n",
            "Actual Caption:\n",
            "two people climbing rocky mountain\n",
            "climbers on steep rock face\n",
            "person in red leading climb\n",
            "mountain climbers near cliff edge\n",
            "group climbing high mountain together\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "man in red shirt is climbing on the mountain\n",
            "\n",
            "Actual Caption:\n",
            "man climbing rocky mountain in sunny day\n",
            "man wearing helmet climbs steep rock face\n",
            "climber with gear moving up on cliff\n",
            "person tackling challenging rocky terrain outdoors\n",
            "athlete climbing up mountain sky background\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "man is sitting on rock rock rock\n",
            "\n",
            "Actual Caption:\n",
            "person climbing steep rocky mountain\n",
            "climber in helmet and gear on rock face\n",
            "rockclimbing adventure on large cliff\n",
            "man wearing green shirt climbing rock\n",
            "solo climber ascending tall mountain\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "man is climbing rock on the mountain\n",
            "\n",
            "Actual Caption:\n",
            "man climbing rocky mountain in gear outdoors\n",
            "climber wearing blue jacket scaling steep rock\n",
            "person holding rope attempting rock climb outside\n",
            "adult against gray cliff with safety equipment on\n",
            "athlete moves up rugged terrain under cloudy sky\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "man in red shirt is climbing rock\n",
            "\n",
            "Actual Caption:\n",
            "woman climbing steep rock face outdoors\n",
            "person holding rope climbing rugged cliff\n",
            "female athlete climbing up mountain surface\n",
            "rock climber moving up rocky wall outside\n",
            "climber in harness ascending steep stone pathway\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "man climbing rock rock\n",
            "\n",
            "Actual Caption:\n",
            "woman climbing rock face outdoors\n",
            "person ascending steep rocky cliff\n",
            "female rock climber on mountain\n",
            "adult wearing black climbing high\n",
            "athlete gripping cliff facing upward\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "man in red shirt climbing on rock\n",
            "\n",
            "Actual Caption:\n",
            "girl climbing rock wearing helmet smiling\n",
            "child with harness climbing steep rocky face\n",
            "young climber on mountain path with rope\n",
            "happy kid wearing red helmet climbing outdoors\n",
            "little climber ascending large cliff with trees behind\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "man is climbing up rock\n",
            "\n",
            "Actual Caption:\n",
            "person climbing rocky mountain against snowy background\n",
            "climber in red jacket on steep rock face snowcovered peaks behind\n",
            "man ascending large cliff with rope snow and blue sky above\n",
            "person in yellow helmet climbing high mountain with ice in distance\n",
            "rock climber moving up rough terrain wearing red mountain backdrop\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "man in red shirt is climbing rock\n",
            "\n",
            "Actual Caption:\n",
            "woman climbing rocky mountain wearing blue helmet\n",
            "person on rock face wearing harness climbing\n",
            "climber ascending steep cliff above tree line\n",
            "outdoor climbing activity on mountain with blue sky\n",
            "young female scaling cliff with safety gear on\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "man in white shirt is climbing rock\n",
            "\n",
            "Actual Caption:\n",
            "man climbing steep rocky mountain\n",
            "person wearing helmet on cliff face\n",
            "rock climber with gear on mountain\n",
            "adventure climbing rock near green trees\n",
            "adult scaling cliff in mountain forest\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "person is climbing on rock\n",
            "\n",
            "Actual Caption:\n",
            "woman climbing rock in sunny outdoor area\n",
            "person ascending steep cliff wearing colorful outfit\n",
            "climber with harness on rocky mountain side\n",
            "female athlete tackling challenging rocky ascent\n",
            "action shot of climber moving up cliff\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "man is climbing on rock\n",
            "\n",
            "Actual Caption:\n",
            "three people climbing rocky mountain side\n",
            "climbers wearing helmets scaling steep cliff\n",
            "group hiking up rocky terrain together\n",
            "persons attached to ropes climb grey rock\n",
            "team wearing blue yellow and grey climb outdoors\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "man is riding bike on dirt mountain\n",
            "\n",
            "Actual Caption:\n",
            "two men climbing steep rocky mountain\n",
            "men wearing helmets climbing red rock\n",
            "climbers with gear ascending rock face\n",
            "group of two on mountain climbing\n",
            "adult climbers tackling steep rock outdoors\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "man in red shirt climbing rock\n",
            "\n",
            "Actual Caption:\n",
            "two people climbing large rocky mountain\n",
            "man and woman wearing helmets climbing rock\n",
            "climbers with safety gear on steep cliff\n",
            "outdoor rock climbing by two adventurous people\n",
            "pair equipped with ropes ascend rocky face\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "man in red shirt is climbing rock\n",
            "\n",
            "Actual Caption:\n",
            "person climbing rocky mountain in sunny day\n",
            "man wearing helmet climbing steep rock face\n",
            "climber in orange helmet ascending large boulder\n",
            "adult scaling rock cliff with climbing gear\n",
            "male athlete tackling rocky cliff outdoors\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "snowboarder is in red shirt is riding the snow\n",
            "\n",
            "Actual Caption:\n",
            "man snowboarding down snowy hill\n",
            "snowboarder wearing blue and black riding slope\n",
            "person performing snowboarding trick on mountain\n",
            "individual sliding on snow covered ground on board\n",
            "snowboarder in blue jacket making sharp turn\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "snowboarder in red shirt is jumping down the snow\n",
            "\n",
            "Actual Caption:\n",
            "man snowboarding down snowy hill wearing orange and gray\n",
            "snowboarder in orange sliding on snowcovered slope\n",
            "person riding snowboard on slope wearing helmet and goggles\n",
            "snowcovered mountain with snowboarder wearing bright jacket\n",
            "action shot of person snowboarding in snow with trees behind\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "snowboarder is in the snow\n",
            "\n",
            "Actual Caption:\n",
            "man snowboarding down snowy hill\n",
            "snowboarder in air wearing green jacket\n",
            "person snowboarding snowy mountain background\n",
            "young athlete snowboarding on slope\n",
            "snowboard action shot on snowy field\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "snowboarder in the air on the snow\n",
            "\n",
            "Actual Caption:\n",
            "person snowboarding down snowy mountain slope\n",
            "snowboarder wearing helmet jumping on snow\n",
            "man in air snowboarding on mountain\n",
            "person performing jump on snowy hill\n",
            "snowboarder midair against steep snowy incline\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "snowboarder in red jacket is is standing down the snow\n",
            "\n",
            "Actual Caption:\n",
            "man snowboarding down snowy hill wearing helmet\n",
            "person in colorful gear sliding on snow\n",
            "snowboarder moving fast on snowy slope\n",
            "adult riding snowboard on snowy mountain\n",
            "young male enjoying snowboarding on sunny day\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "person in the snow in the snow\n",
            "\n",
            "Actual Caption:\n",
            "person snowboarding on snowy mountain\n",
            "man wearing helmet standing on snowboard\n",
            "snowboarder preparing to move down slope\n",
            "young adult dressed for snowboarding outdoors\n",
            "snow falls around snowboarder on mountain\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "two people are walking on snowy snow\n",
            "\n",
            "Actual Caption:\n",
            "two people snowboarding down snowy hill\n",
            "snowboarders sliding on snowcovered slope\n",
            "person in front snowboarding on mountain\n",
            "snowy mountain with snowboarders in action\n",
            "two snowboarders enjoying sunny day on snow\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "man in red jacket is is skiing in the snow\n",
            "\n",
            "Actual Caption:\n",
            "person snowboarding down snowy mountain slope\n",
            "woman in pink jacket snowboarding on snowy hill\n",
            "snowboarder sliding down snowy slope trees in background\n",
            "young snowboarder wearing bright jacket on snowy field\n",
            "adult snowboarding on snowcovered hill wearing helmet\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "man is jumping down the air\n",
            "\n",
            "Actual Caption:\n",
            "person snowboarding blue sky background\n",
            "snowboarder jumping snowy hill beneath\n",
            "man performing jump on snowy slope\n",
            "snowboarder in air clear sunny day\n",
            "action shot of snowboarding on mountain\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "snowboarder is in red and white jacket and white jacket is skiing down the snow\n",
            "\n",
            "Actual Caption:\n",
            "snowboarding man jumping high on snowy hill\n",
            "man wearing helmet doing snowboard jump\n",
            "snowboarder flying through air over snow\n",
            "person in midair while snowboarding down slope\n",
            "action shot of snowboarder jumping in mountains\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "snowboarder in red jacket is jumping on the snow\n",
            "\n",
            "Actual Caption:\n",
            "man snowboarding down snowy mountain\n",
            "snowboarder sliding on snowcovered slope\n",
            "person wearing black and white on snowboard\n",
            "man performing snowboarding trick on mountain\n",
            "snowboarder in black pants riding on snowy hill\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "man in red jacket is standing on the snow\n",
            "\n",
            "Actual Caption:\n",
            "snowboarding man standing on snowy mountain at sunset\n",
            "person wearing goggles on snowcovered slope with trees\n",
            "snowboarder in black outfit pausing on hill during sunrise\n",
            "man with snowboard watching sunrise on snowy mountain top\n",
            "snowboarder enjoying sunset standing alone on snow field\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "man in red jacket is skiing on the snow\n",
            "\n",
            "Actual Caption:\n",
            "woman standing on snow with snowboard sunny day\n",
            "girl wearing helmet goggles ready to snowboard\n",
            "female snowboarder on snowy mountain clear sky\n",
            "young person at snowy hill snowboarding gear on\n",
            "snowboarder ready to slide downhill snowy landscape\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "person is skiing down snowy hill\n",
            "\n",
            "Actual Caption:\n",
            "person snowboarding down snowy hill\n",
            "snowboarder sliding on snow creating spray\n",
            "man wearing dark outfit snowboarding\n",
            "snowboarder making turn on snowy slope\n",
            "action shot of snowboarder on mountain\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "man in black jacket is walking down snowy snow\n",
            "\n",
            "Actual Caption:\n",
            "person snowboarding down snowy mountain\n",
            "man wearing helmet moving on snow\n",
            "snowboarder sliding on mountain cloudy backdrop\n",
            "snowboarder wearing blue jacket riding alone\n",
            "young person snowy slope winter sport\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "man in red jacket is standing down snowy hill\n",
            "\n",
            "Actual Caption:\n",
            "man in yellow and blue snowboarding on snowy hill\n",
            "snowboarder wearing goggles sliding down snowcovered slope\n",
            "young person enjoying snowboard ride down snowy mountain\n",
            "snowboarder with blue jacket on snowy slope in sunlight\n",
            "person snowboarding wearing helmet gloves and bright suit\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "person in red jacket is skiing\n",
            "\n",
            "Actual Caption:\n",
            "man snowboarding down snowy mountain\n",
            "person wearing red skiing on slope\n",
            "skier in snowy area near ski lift\n",
            "snowboarder gliding on snowcovered hill\n",
            "man on snowboard descending mountain fast\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "person in red jacket and red jacket\n",
            "\n",
            "Actual Caption:\n",
            "man wearing grey jacket snowboarding down snowy hill\n",
            "person in dark outfit gliding on snowcovered slope\n",
            "snowboarder descending mountain with trees in background\n",
            "man with snowboard moving down white snowy terrain\n",
            "snowboard rider in helmet and goggles enjoying winter sport\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "man in red and white jacket is walking down the snow\n",
            "\n",
            "Actual Caption:\n",
            "person standing on snowy hill wearing helmet\n",
            "snowboarder ready to ride down snowy mountain\n",
            "snowboarder looking at distance before descending\n",
            "man in snow gear preparing for snowboarding\n",
            "snowboarder pausing on slope trees in background\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "man in white shirt is standing in the snow\n",
            "\n",
            "Actual Caption:\n",
            "man standing on snowboard looking down snowy mountain\n",
            "snowboarder at top of snowy slope ready to descend\n",
            "person in grey jacket prepares to snowboard down hill\n",
            "snowcovered mountain with snowboarder enjoying view\n",
            "young man snowboarding standing atop mountain slope\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "person in blue jacket is jumping down the snow\n",
            "\n",
            "Actual Caption:\n",
            "woman snowboarding downhill on snowy mountain\n",
            "person wearing black sliding down snowcovered slope\n",
            "snowboarder in motion across white snow with trees\n",
            "young female enjoying snowboarding on sunny day\n",
            "athlete riding snowboard on steep mountain side\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "man in blue jacket and red shirt is standing on the snow\n",
            "\n",
            "Actual Caption:\n",
            "man standing on snowy hill with snowboard\n",
            "man in blue pants and checkered jacket skiing\n",
            "snowboarder on snowy slope wearing helmet\n",
            "person preparing to snowboard down mountain\n",
            "snowcovered mountain with snowboarding man\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "person in the snow\n",
            "\n",
            "Actual Caption:\n",
            "man skiing down snowy mountain in red jacket\n",
            "skier in red descending snowy slope with backpack\n",
            "person skiing on snowy hill under ski lifts\n",
            "snowcovered slope with skier in red and black\n",
            "redjacketed skier making tracks on mountain snow\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "man in red jacket is skiing down the snow\n",
            "\n",
            "Actual Caption:\n",
            "man snowboarding down snowy mountain slope\n",
            "person wearing blue jacket rides snowboard\n",
            "snowboarder carving through snow on hill\n",
            "action shot of snowboarder in motion\n",
            "athlete snowboarding on snowy slope midair\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Predicted Caption:\n",
            "man in red is skiing in the snow\n",
            "\n",
            "Actual Caption:\n",
            "man skiing down snowy mountain slope\n",
            "person in red pants snowboarding\n",
            "skier moving fast on powder snow\n",
            "snowcovered slope with downhill skier\n",
            "man wearing helmet skiing in snow\n",
            "------------------------------------\n",
            "bleu_1 = 27.75, bleu_2 = 8.43, bleu_3 = 1.98\n"
          ]
        }
      ],
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "def evaluate_bleu(encoder_decoder, test_dataloader, device, subset):\n",
        "    max_len = 60\n",
        "    sos_id, eos_id, pad_id = subset._start_idx, subset._end_idx, subset._pad_idx\n",
        "    idx2word = subset._idx2word\n",
        "\n",
        "    bleu_w = {\n",
        "        \"bleu-1\": [1.0],\n",
        "        \"bleu-2\": [0.5, 0.5],\n",
        "        \"bleu-3\": [0.333, 0.333, 0.333],\n",
        "    }\n",
        "\n",
        "    references_total = []\n",
        "    predictions_total = []\n",
        "\n",
        "    bleu_1_overall, bleu_2_overall, bleu_3_overall = 0, 0, 0\n",
        "\n",
        "    print(\"Evaluating model.\")\n",
        "\n",
        "    c = 0\n",
        "    for x_img_batched, x_img_raw, caption_words_list in test_dataloader:\n",
        "        x_img_batched = x_img_batched.to(device)\n",
        "        img_features = encoder_decoder.forward_encoder(x_img_batched).detach()\n",
        "        predictions = greedy_decoding(encoder_decoder, img_features, sos_id, eos_id, pad_id, idx2word, max_len, device)\n",
        "        prediction = predictions[0]\n",
        "        prediction = [string for string in prediction if \"<unk>\" not in string]\n",
        "\n",
        "        bleu_1_image, bleu_2_image, bleu_3_image = 0, 0, 0\n",
        "\n",
        "        list_of_lists_of_words = [[word[0] if isinstance(word, tuple) else word for word in sublist] for sublist in caption_words_list]\n",
        "        print(\"------------------------------------\")\n",
        "        print(\"Predicted Caption:\")\n",
        "        print(\" \".join(prediction))\n",
        "        print()\n",
        "        print(\"Actual Caption:\")\n",
        "        for caption in list_of_lists_of_words:\n",
        "            print(\" \".join(caption))\n",
        "            bleu_1_image = max(bleu_1_image, sentence_bleu([caption], prediction, weights=bleu_w[\"bleu-1\"]) * 100)\n",
        "            bleu_2_image = max(bleu_2_image, sentence_bleu([caption], prediction, weights=bleu_w[\"bleu-2\"]) * 100)\n",
        "            bleu_3_image = max(bleu_3_image, sentence_bleu([caption], prediction, weights=bleu_w[\"bleu-3\"]) * 100)\n",
        "        print(\"------------------------------------\")\n",
        "        bleu_1_overall += bleu_1_image\n",
        "        bleu_2_overall += bleu_2_image\n",
        "        bleu_3_overall += bleu_3_image\n",
        "        c += 1\n",
        "\n",
        "    bleu_1_overall /= c\n",
        "    bleu_2_overall /= c\n",
        "    bleu_3_overall /= c\n",
        "\n",
        "    print(f\"bleu_1 = {bleu_1_overall:.2f}, bleu_2 = {bleu_2_overall:.2f}, bleu_3 = {bleu_3_overall:.2f}\")\n",
        "\n",
        "test_dataset = TestDataset(split_save[\"test\"])\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "evaluate_bleu(encoder_decoder, test_dataloader, device, train_set)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ad6de0cf34b94316ac2e2888593fbc7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_781d9716a4d646f0ab70d24e2823dc1e",
              "IPY_MODEL_5903abd570ad4eacb9ddff56b4ae7597",
              "IPY_MODEL_819866ddf5084386a2e0b7123d640401"
            ],
            "layout": "IPY_MODEL_797b8c938cf4439ebdcd26b4a6ebfccb"
          }
        },
        "781d9716a4d646f0ab70d24e2823dc1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee406212b1c04378b78919efb593d545",
            "placeholder": "​",
            "style": "IPY_MODEL_07345f587d9948aea1502889f0c7c0d6",
            "value": "model.safetensors: 100%"
          }
        },
        "5903abd570ad4eacb9ddff56b4ae7597": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8e3772673a94d4fab122e738e3c7ecb",
            "max": 268076984,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_408540d767b444898c104a2fc73ecc94",
            "value": 268076984
          }
        },
        "819866ddf5084386a2e0b7123d640401": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_602bc65c29534df2b1acde44608c86cb",
            "placeholder": "​",
            "style": "IPY_MODEL_c7a9d0a99bcc42378881616cb629818d",
            "value": " 268M/268M [00:02&lt;00:00, 113MB/s]"
          }
        },
        "797b8c938cf4439ebdcd26b4a6ebfccb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee406212b1c04378b78919efb593d545": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07345f587d9948aea1502889f0c7c0d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a8e3772673a94d4fab122e738e3c7ecb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "408540d767b444898c104a2fc73ecc94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "602bc65c29534df2b1acde44608c86cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7a9d0a99bcc42378881616cb629818d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}